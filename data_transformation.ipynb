{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4f212f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Loading source tables...\n",
      "fact columns before calculating EXTENDED_PRICE: ['INVOICELINEID', 'INVOICEID', 'TRACKID', 'UNITPRICE_x', 'QUANTITY', 'CUSTOMERID', 'INVOICE_DATE', 'BILLINGADDRESS', 'BILLINGCITY', 'BILLINGSTATE', 'BILLINGCOUNTRY', 'BILLINGPOSTALCODE', 'TOTAL', 'FIRSTNAME_x', 'LASTNAME_x', 'COMPANY', 'ADDRESS_x', 'CITY_x', 'STATE_x', 'COUNTRY_x', 'POSTALCODE_x', 'PHONE_x', 'FAX_x', 'EMAIL_x', 'SUPPORTREPID', 'EMPLOYEEID', 'LASTNAME_y', 'FIRSTNAME_y', 'TITLE_x', 'REPORTSTO', 'BIRTHDATE', 'HIREDATE', 'ADDRESS_y', 'CITY_y', 'STATE_y', 'COUNTRY_y', 'POSTALCODE_y', 'PHONE_y', 'FAX_y', 'EMAIL_y', 'NAME_x', 'ALBUMID', 'MEDIATYPEID', 'GENREID', 'COMPOSER', 'MILLISECONDS', 'BYTES', 'UNITPRICE_y', 'TITLE_y', 'ARTISTID', 'NAME_y', 'LOCATION_ID', 'CITY', 'STATE', 'COUNTRY', 'POSTALCODE']\n",
      "‚ö†Ô∏è UNIT_PRICE column is missing! Check if renaming was done correctly.\n",
      "‚úÖ All dimensions and facts loaded into ERD_SCHEMA_STAR successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from snowflake.snowpark import Session\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Snowflake connection configuration\n",
    "snowflake_params = {\n",
    "    \"account\": os.getenv(\"SNOWFLAKE_ACCOUNT\"),\n",
    "    \"user\": os.getenv(\"SNOWFLAKE_USER\"),\n",
    "    \"password\": os.getenv(\"SNOWFLAKE_PASSWORD\"),\n",
    "    \"role\": os.getenv(\"SNOWFLAKE_ROLE\"),\n",
    "    \"warehouse\": os.getenv(\"SNOWFLAKE_WAREHOUSE\"),\n",
    "    \"database\": os.getenv(\"SNOWFLAKE_DATABASE\"),\n",
    "    \"schema\": \"ERD_SCHEMA_CLEANED\"\n",
    "}\n",
    "final_schema = os.getenv(\"SNOWFLAKE_STAR_SCHEMA\")\n",
    "\n",
    "# Initialize session\n",
    "session = Session.builder.configs(snowflake_params).create()\n",
    "\n",
    "def set_schema(schema):\n",
    "    session.sql(f'USE SCHEMA \"{snowflake_params[\"database\"]}\".\"{schema}\"').collect()\n",
    "\n",
    "# Load source tables\n",
    "print(\"üîÑ Loading source tables...\")\n",
    "df_customer = session.table(\"CUSTOMER\").to_pandas()\n",
    "df_employee = session.table(\"EMPLOYEE\").to_pandas()\n",
    "df_artist = session.table(\"ARTIST\").to_pandas()\n",
    "df_album = session.table(\"ALBUM\").to_pandas()\n",
    "df_invoice = session.table(\"INVOICE\").to_pandas()\n",
    "df_invoiceline = session.table(\"INVOICELINE\").to_pandas()\n",
    "df_track = session.table(\"TRACK\").to_pandas()\n",
    "df_playlisttrack = session.table(\"PLAYLISTTRACK\").to_pandas()\n",
    "df_genre = session.table(\"GENRE\").to_pandas()\n",
    "df_mediatype = session.table(\"MEDIATYPE\").to_pandas()\n",
    "df_playlist = session.table(\"PLAYLIST\").to_pandas()\n",
    "\n",
    "# Ensure that columns exist and are correctly typed\n",
    "def ensure_int_key(df, col):\n",
    "    \"\"\"Ensure that a column exists and is converted to an integer, handling errors.\"\"\"\n",
    "    if col in df.columns:\n",
    "        try:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0).astype(int)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Error while converting {col} to int: {e}\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è Column {col} is missing in the DataFrame.\")\n",
    "    return df\n",
    "\n",
    "# Fact table creation logic\n",
    "def create_fact_sales(df_invoiceline, df_invoice, df_track, df_album, df_artist,\n",
    "                      df_customer, df_employee, df_playlisttrack, dim_location):\n",
    "    # Uppercase columns for consistency\n",
    "    df_invoiceline.columns = df_invoiceline.columns.str.upper()\n",
    "    df_invoice.columns = df_invoice.columns.str.upper()\n",
    "    df_track.columns = df_track.columns.str.upper()\n",
    "    df_album.columns = df_album.columns.str.upper()\n",
    "    df_artist.columns = df_artist.columns.str.upper()\n",
    "    df_customer.columns = df_customer.columns.str.upper()\n",
    "    df_employee.columns = df_employee.columns.str.upper()\n",
    "\n",
    "    # Rename customer billing address columns for location join\n",
    "    df_customer.rename(columns={\n",
    "        'BILLINGCITY': 'CITY',\n",
    "        'BILLINGSTATE': 'STATE',\n",
    "        'BILLINGCOUNTRY': 'COUNTRY',\n",
    "        'BILLINGPOSTALCODE': 'POSTALCODE'\n",
    "    }, inplace=True)\n",
    "\n",
    "    # Ensure key columns are integers for joining\n",
    "    for col in ['INVOICEID', 'TRACKID', 'CUSTOMERID', 'SUPPORTREPID']:\n",
    "        if col in df_invoiceline.columns:\n",
    "            df_invoiceline = ensure_int_key(df_invoiceline, col)\n",
    "    for col in ['INVOICEID', 'CUSTOMERID']:\n",
    "        if col in df_invoice.columns:\n",
    "            df_invoice = ensure_int_key(df_invoice, col)\n",
    "    if 'CUSTOMERID' in df_customer.columns:\n",
    "        df_customer = ensure_int_key(df_customer, 'CUSTOMERID')\n",
    "    if 'SUPPORTREPID' in df_customer.columns:\n",
    "        df_customer = ensure_int_key(df_customer, 'SUPPORTREPID')\n",
    "    for col in ['TRACKID', 'ALBUMID']:\n",
    "        if col in df_track.columns:\n",
    "            df_track = ensure_int_key(df_track, col)\n",
    "    for col in ['ALBUMID', 'ARTISTID']:\n",
    "        if col in df_album.columns:\n",
    "            df_album = ensure_int_key(df_album, col)\n",
    "    if 'ARTISTID' in df_artist.columns:\n",
    "        df_artist = ensure_int_key(df_artist, 'ARTISTID')\n",
    "    if 'EMPLOYEEID' in df_employee.columns:\n",
    "        df_employee = ensure_int_key(df_employee, 'EMPLOYEEID')\n",
    "\n",
    "    # Merge invoices with invoiceline and customer data\n",
    "    df_invoice.rename(columns={'INVOICEDATE': 'INVOICE_DATE', 'UNITPRICE': 'UNIT_PRICE'}, inplace=True)\n",
    "    invoice_subset = df_invoice.drop(columns=['INVOICEDATE'], errors='ignore')\n",
    "    fact = df_invoiceline.merge(invoice_subset, on='INVOICEID', how='left', suffixes=('', '_inv'))\n",
    "\n",
    "    # Resolve duplicate 'UNITPRICE'\n",
    "    if 'UNITPRICE_y' in fact.columns:\n",
    "        fact['UNIT_PRICE'] = fact['UNITPRICE_y']  # Use the merged column\n",
    "        fact = fact.drop(columns=['UNITPRICE_y'])  # Drop the duplicate\n",
    "\n",
    "    if 'UNITPRICE_x' in fact.columns:\n",
    "        fact['UNIT_PRICE'] = fact['UNITPRICE_x']  # Use the original column\n",
    "        fact = fact.drop(columns=['UNITPRICE_x'])  # Drop the duplicate\n",
    "\n",
    "    # Merge with customer and employee\n",
    "    fact = fact.merge(df_customer, on='CUSTOMERID', how='left')\n",
    "    if 'SUPPORTREPID' in df_customer.columns:\n",
    "        fact = fact.merge(df_employee, left_on='SUPPORTREPID', right_on='EMPLOYEEID', how='left')\n",
    "\n",
    "    # Merge with track, album, artist\n",
    "    fact = fact.merge(df_track, on='TRACKID', how='left')\n",
    "    fact = fact.merge(df_album, on='ALBUMID', how='left')\n",
    "    fact = fact.merge(df_artist, on='ARTISTID', how='left')\n",
    "\n",
    "    # Location merge with fallback logic\n",
    "    location_keys = ['CITY', 'STATE', 'COUNTRY', 'POSTALCODE']\n",
    "    invoice_address_keys = ['BILLINGCITY', 'BILLINGSTATE', 'BILLINGCOUNTRY', 'BILLINGPOSTALCODE']\n",
    "    if all(col in fact.columns for col in location_keys):\n",
    "        fact = fact.merge(dim_location, left_on=location_keys, right_on=location_keys, how='left')\n",
    "    elif all(col in fact.columns for col in invoice_address_keys):\n",
    "        fact = fact.merge(dim_location, left_on=invoice_address_keys, right_on=location_keys, how='left')\n",
    "\n",
    "    # Calculate EXTENDED_PRICE\n",
    "    # Debugging: Check if 'UNIT_PRICE' exists before calculating EXTENDED_PRICE\n",
    "    print('fact columns before calculating EXTENDED_PRICE:', fact.columns.tolist())\n",
    "    if 'UNIT_PRICE' not in fact.columns:\n",
    "        print('‚ö†Ô∏è UNIT_PRICE column is missing! Check if renaming was done correctly.')\n",
    "    else:\n",
    "        fact['EXTENDED_PRICE'] = fact['UNIT_PRICE'] * fact['QUANTITY']\n",
    "\n",
    "    # Rename columns for fact table output\n",
    "    rename_map = {\n",
    "        'INVOICELINEID': 'INVOICELINE_ID',\n",
    "        'INVOICEID': 'INVOICE_ID',\n",
    "        'INVOICE_DATE': 'INVOICE_DATE',\n",
    "        'TRACKID': 'TRACK_ID',\n",
    "        'CUSTOMERID': 'CUSTOMER_ID',\n",
    "        'UNIT_PRICE': 'UNIT_PRICE',\n",
    "        'QUANTITY': 'QUANTITY',\n",
    "        'EXTENDED_PRICE': 'EXTENDED_PRICE'\n",
    "    }\n",
    "    if 'SUPPORTREPID' in fact.columns:\n",
    "        rename_map['SUPPORTREPID'] = 'EMPLOYEE_ID'\n",
    "\n",
    "    fact_sales = fact.rename(columns=rename_map)\n",
    "\n",
    "    return fact_sales.reset_index(drop=True)\n",
    "\n",
    "# Proceed with further transformations, save CSVs in .csv.gz format, and load into Snowflake stage\n",
    "\n",
    "# Save transformed dataframes as CSV.gz files\n",
    "fact_sales.to_csv('Dim_Customer_star.csv.gz', index=False, compression='gzip')\n",
    "dim_location.to_csv('Dim_Location_star.csv.gz', index=False, compression='gzip')\n",
    "dim_date.to_csv('Dim_Date_star.csv.gz', index=False, compression='gzip')\n",
    "\n",
    "# Upload CSVs to Snowflake stage FINAL_TRANSFORMED_STAGE (upload manually via Snowflake UI or use PUT command)\n",
    "dim_date = create_dim_date(df_invoice)\n",
    "dim_location = create_dim_location(df_customer)\n",
    "dim_album_artist = create_dim_album_artist(df_album, df_artist)\n",
    "dim_track = create_dim_track(df_track, df_genre, df_mediatype)\n",
    "dim_playlist_track = create_dim_playlist_track(df_playlisttrack, df_playlist)\n",
    "dim_employee = create_dim_employee(df_employee)\n",
    "dim_customer = create_dim_customer(df_customer)\n",
    "\n",
    "fact_sales = create_fact_sales(df_invoiceline, df_invoice, df_track, df_album, df_artist,\n",
    "                               df_customer, df_employee, df_playlisttrack, dim_location)\n",
    "\n",
    "# Switch to final schema\n",
    "set_schema(final_schema)\n",
    "#\n",
    "print(\"‚úÖ All dimensions and facts loaded into ERD_SCHEMA_STAR successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770a6125",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from snowflake.snowpark import Session\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Snowflake connection configuration\n",
    "snowflake_params = {\n",
    "    \"account\": os.getenv(\"SNOWFLAKE_ACCOUNT\"),\n",
    "    \"user\": os.getenv(\"SNOWFLAKE_USER\"),\n",
    "    \"password\": os.getenv(\"SNOWFLAKE_PASSWORD\"),\n",
    "    \"role\": os.getenv(\"SNOWFLAKE_ROLE\"),\n",
    "    \"warehouse\": os.getenv(\"SNOWFLAKE_WAREHOUSE\"),\n",
    "    \"database\": os.getenv(\"SNOWFLAKE_DATABASE\"),\n",
    "    \"schema\": \"ERD_SCHEMA_CLEANED\"\n",
    "}\n",
    "final_schema = os.getenv(\"SNOWFLAKE_STAR_SCHEMA\")\n",
    "\n",
    "# Initialize session\n",
    "session = Session.builder.configs(snowflake_params).create()\n",
    "\n",
    "def set_schema(schema):\n",
    "    session.sql(f'USE SCHEMA \"{snowflake_params[\"database\"]}\".\"{schema}\"').collect()\n",
    "\n",
    "# Load source tables\n",
    "print(\"üîÑ Loading source tables...\")\n",
    "df_customer = session.table(\"CUSTOMER\").to_pandas()\n",
    "df_employee = session.table(\"EMPLOYEE\").to_pandas()\n",
    "df_artist = session.table(\"ARTIST\").to_pandas()\n",
    "df_album = session.table(\"ALBUM\").to_pandas()\n",
    "df_invoice = session.table(\"INVOICE\").to_pandas()\n",
    "df_invoiceline = session.table(\"INVOICELINE\").to_pandas()\n",
    "df_track = session.table(\"TRACK\").to_pandas()\n",
    "df_playlisttrack = session.table(\"PLAYLISTTRACK\").to_pandas()\n",
    "df_genre = session.table(\"GENRE\").to_pandas()\n",
    "df_mediatype = session.table(\"MEDIATYPE\").to_pandas()\n",
    "df_playlist = session.table(\"PLAYLIST\").to_pandas()\n",
    "\n",
    "# Ensure that columns exist and are correctly typed\n",
    "def ensure_int_key(df, col):\n",
    "    \"\"\"Ensure that a column exists and is converted to an integer, handling errors.\"\"\"\n",
    "    if col in df.columns:\n",
    "        try:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0).astype(int)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Error while converting {col} to int: {e}\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è Column {col} is missing in the DataFrame.\")\n",
    "    return df\n",
    "\n",
    "# Fact table creation logic\n",
    "def create_fact_sales(df_invoiceline, df_invoice, df_track, df_album, df_artist,\n",
    "                      df_customer, df_employee, df_playlisttrack, dim_location):\n",
    "    # Uppercase columns for consistency\n",
    "    df_invoiceline.columns = df_invoiceline.columns.str.upper()\n",
    "    df_invoice.columns = df_invoice.columns.str.upper()\n",
    "    df_track.columns = df_track.columns.str.upper()\n",
    "    df_album.columns = df_album.columns.str.upper()\n",
    "    df_artist.columns = df_artist.columns.str.upper()\n",
    "    df_customer.columns = df_customer.columns.str.upper()\n",
    "    df_employee.columns = df_employee.columns.str.upper()\n",
    "\n",
    "    # Rename customer billing address columns for location join\n",
    "    df_customer.rename(columns={\n",
    "        'BILLINGCITY': 'CITY',\n",
    "        'BILLINGSTATE': 'STATE',\n",
    "        'BILLINGCOUNTRY': 'COUNTRY',\n",
    "        'BILLINGPOSTALCODE': 'POSTALCODE'\n",
    "    }, inplace=True)\n",
    "\n",
    "    # Ensure key columns are integers for joining\n",
    "    for col in ['INVOICEID', 'TRACKID', 'CUSTOMERID', 'SUPPORTREPID']:\n",
    "        if col in df_invoiceline.columns:\n",
    "            df_invoiceline = ensure_int_key(df_invoiceline, col)\n",
    "    for col in ['INVOICEID', 'CUSTOMERID']:\n",
    "        if col in df_invoice.columns:\n",
    "            df_invoice = ensure_int_key(df_invoice, col)\n",
    "    if 'CUSTOMERID' in df_customer.columns:\n",
    "        df_customer = ensure_int_key(df_customer, 'CUSTOMERID')\n",
    "    if 'SUPPORTREPID' in df_customer.columns:\n",
    "        df_customer = ensure_int_key(df_customer, 'SUPPORTREPID')\n",
    "    for col in ['TRACKID', 'ALBUMID']:\n",
    "        if col in df_track.columns:\n",
    "            df_track = ensure_int_key(df_track, col)\n",
    "    for col in ['ALBUMID', 'ARTISTID']:\n",
    "        if col in df_album.columns:\n",
    "            df_album = ensure_int_key(df_album, col)\n",
    "    if 'ARTISTID' in df_artist.columns:\n",
    "        df_artist = ensure_int_key(df_artist, 'ARTISTID')\n",
    "    if 'EMPLOYEEID' in df_employee.columns:\n",
    "        df_employee = ensure_int_key(df_employee, 'EMPLOYEEID')\n",
    "\n",
    "    # Merge invoices with invoiceline and customer data\n",
    "    df_invoice.rename(columns={'INVOICEDATE': 'INVOICE_DATE', 'UNITPRICE': 'UNIT_PRICE'}, inplace=True)\n",
    "    invoice_subset = df_invoice.drop(columns=['INVOICEDATE'], errors='ignore')\n",
    "    fact = df_invoiceline.merge(invoice_subset, on='INVOICEID', how='left', suffixes=('', '_inv'))\n",
    "\n",
    "    # Resolve duplicate 'UNITPRICE'\n",
    "    if 'UNITPRICE_y' in fact.columns:\n",
    "        fact['UNIT_PRICE'] = fact['UNITPRICE_y']  # Use the merged column\n",
    "        fact = fact.drop(columns=['UNITPRICE_y'])  # Drop the duplicate\n",
    "\n",
    "    if 'UNITPRICE_x' in fact.columns:\n",
    "        fact['UNIT_PRICE'] = fact['UNITPRICE_x']  # Use the original column\n",
    "        fact = fact.drop(columns=['UNITPRICE_x'])  # Drop the duplicate\n",
    "\n",
    "    # Merge with customer and employee\n",
    "    fact = fact.merge(df_customer, on='CUSTOMERID', how='left')\n",
    "    if 'SUPPORTREPID' in df_customer.columns:\n",
    "        fact = fact.merge(df_employee, left_on='SUPPORTREPID', right_on='EMPLOYEEID', how='left')\n",
    "\n",
    "    # Merge with track, album, artist\n",
    "    fact = fact.merge(df_track, on='TRACKID', how='left')\n",
    "    fact = fact.merge(df_album, on='ALBUMID', how='left')\n",
    "    fact = fact.merge(df_artist, on='ARTISTID', how='left')\n",
    "\n",
    "    # Location merge with fallback logic\n",
    "    location_keys = ['CITY', 'STATE', 'COUNTRY', 'POSTALCODE']\n",
    "    invoice_address_keys = ['BILLINGCITY', 'BILLINGSTATE', 'BILLINGCOUNTRY', 'BILLINGPOSTALCODE']\n",
    "    if all(col in fact.columns for col in location_keys):\n",
    "        fact = fact.merge(dim_location, left_on=location_keys, right_on=location_keys, how='left')\n",
    "    elif all(col in fact.columns for col in invoice_address_keys):\n",
    "        fact = fact.merge(dim_location, left_on=invoice_address_keys, right_on=location_keys, how='left')\n",
    "\n",
    "    # Calculate EXTENDED_PRICE\n",
    "    if 'UNIT_PRICE' in fact.columns:\n",
    "        fact['EXTENDED_PRICE'] = fact['UNIT_PRICE'] * fact['QUANTITY']\n",
    "\n",
    "    # Rename columns for fact table output\n",
    "    rename_map = {\n",
    "        'INVOICELINEID': 'INVOICELINE_ID',\n",
    "        'INVOICEID': 'INVOICE_ID',\n",
    "        'INVOICE_DATE': 'INVOICE_DATE',\n",
    "        'TRACKID': 'TRACK_ID',\n",
    "        'CUSTOMERID': 'CUSTOMER_ID',\n",
    "        'UNIT_PRICE': 'UNIT_PRICE',\n",
    "        'QUANTITY': 'QUANTITY',\n",
    "        'EXTENDED_PRICE': 'EXTENDED_PRICE'\n",
    "    }\n",
    "    if 'SUPPORTREPID' in fact.columns:\n",
    "        rename_map['SUPPORTREPID'] = 'EMPLOYEE_ID'\n",
    "\n",
    "    fact_sales = fact.rename(columns=rename_map)\n",
    "\n",
    "    # Keep only relevant columns in fact_sales\n",
    "    columns_to_keep = [\n",
    "        'INVOICELINE_ID', 'LOCATION_ID', 'INVOICE_ID', 'TRACK_ID', 'CUSTOMER_ID',\n",
    "        'EMPLOYEE_ID', 'MEDIATYPE_ID', 'INVOICE_DATE', 'UNIT_PRICE', 'QUANTITY',\n",
    "        'TOTAL_AMOUNT', 'PLAYLIST_ID', 'ALBUM_ID', 'ARTIST_ID', 'MILLISECONDS', 'BYTES'\n",
    "    ]\n",
    "    fact_sales = fact_sales[columns_to_keep]\n",
    "\n",
    "    return fact_sales.reset_index(drop=True)\n",
    "\n",
    "# Proceed with further transformations, save CSVs in .csv.gz format, and load into Snowflake stage\n",
    "\n",
    "# Save transformed dataframes as CSV.gz files\n",
    "fact_sales.to_csv('Fact_Sales_star.csv.gz', index=False, compression='gzip')\n",
    "dim_location.to_csv('Dim_Location_star.csv.gz', index=False, compression='gzip')\n",
    "dim_date.to_csv('Dim_Date_star.csv.gz', index=False, compression='gzip')\n",
    "dim_album_artist.to_csv('Dim_Album_Artist_star.csv.gz', index=False, compression='gzip')\n",
    "dim_track.to_csv('Dim_Track_star.csv.gz', index=False, compression='gzip')\n",
    "dim_playlist_track.to_csv('Dim_Playlist_Track_star.csv.gz', index=False, compression='gzip')\n",
    "dim_employee.to_csv('Dim_Employee_star.csv.gz', index=False, compression='gzip')\n",
    "dim_customer.to_csv('Dim_Customer_star.csv.gz', index=False, compression='gzip')\n",
    "\n",
    "# Upload CSVs to Snowflake stage FINAL_TRANSFORMED_STAGE (upload manually via Snowflake UI or use PUT command)\n",
    "dim_date = create_dim_date(df_invoice)\n",
    "dim_location = create_dim_location(df_customer)\n",
    "dim_album_artist = create_dim_album_artist(df_album, df_artist)\n",
    "dim_track = create_dim_track(df_track, df_genre, df_mediatype)\n",
    "dim_playlist_track = create_dim_playlist_track(df_playlisttrack, df_playlist)\n",
    "dim_employee = create_dim_employee(df_employee)\n",
    "dim_customer = create_dim_customer(df_customer)\n",
    "\n",
    "fact_sales = create_fact_sales(df_invoiceline, df_invoice, df_track, df_album, df_artist,\n",
    "                               df_customer, df_employee, df_playlisttrack, dim_location)\n",
    "\n",
    "# Switch to final schema\n",
    "set_schema(final_schema)\n",
    "\n",
    "def load_df_to_table(df, table_name):\n",
    "    print(f\"\\nLoading {table_name}...\")\n",
    "    print(f\"{table_name} rows: {len(df)}\")\n",
    "    print(df.head())\n",
    "    # Data loadingIt seems like you've shared a code snippet, and I understand that you're facing some issues with the final steps related to saving the CSVs, uploading them to Snowflake, and loading the data into the final Snowflake tables. I‚Äôll summarize and update the code to make sure it works as intended.\n",
    "\n",
    "Here‚Äôs an **updated, cleaned version** of your code with corrections:\n",
    "\n",
    "### Complete Code:\n",
    "\n",
    "```python\n",
    "import os\n",
    "import pandas as pd\n",
    "from snowflake.snowpark import Session\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Snowflake connection configuration\n",
    "snowflake_params = {\n",
    "    \"account\": os.getenv(\"SNOWFLAKE_ACCOUNT\"),\n",
    "    \"user\": os.getenv(\"SNOWFLAKE_USER\"),\n",
    "    \"password\": os.getenv(\"SNOWFLAKE_PASSWORD\"),\n",
    "    \"role\": os.getenv(\"SNOWFLAKE_ROLE\"),\n",
    "    \"warehouse\": os.getenv(\"SNOWFLAKE_WAREHOUSE\"),\n",
    "    \"database\": os.getenv(\"SNOWFLAKE_DATABASE\"),\n",
    "    \"schema\": \"ERD_SCHEMA_CLEANED\"\n",
    "}\n",
    "final_schema = os.getenv(\"SNOWFLAKE_STAR_SCHEMA\")\n",
    "\n",
    "# Initialize session\n",
    "session = Session.builder.configs(snowflake_params).create()\n",
    "\n",
    "def set_schema(schema):\n",
    "    session.sql(f'USE SCHEMA \"{snowflake_params[\"database\"]}\".\"{schema}\"').collect()\n",
    "\n",
    "# Load source tables\n",
    "print(\"üîÑ Loading source tables...\")\n",
    "df_customer = session.table(\"CUSTOMER\").to_pandas()\n",
    "df_employee = session.table(\"EMPLOYEE\").to_pandas()\n",
    "df_artist = session.table(\"ARTIST\").to_pandas()\n",
    "df_album = session.table(\"ALBUM\").to_pandas()\n",
    "df_invoice = session.table(\"INVOICE\").to_pandas()\n",
    "df_invoiceline = session.table(\"INVOICELINE\").to_pandas()\n",
    "df_track = session.table(\"TRACK\").to_pandas()\n",
    "df_playlisttrack = session.table(\"PLAYLISTTRACK\").to_pandas()\n",
    "df_genre = session.table(\"GENRE\").to_pandas()\n",
    "df_mediatype = session.table(\"MEDIATYPE\").to_pandas()\n",
    "df_playlist = session.table(\"PLAYLIST\").to_pandas()\n",
    "\n",
    "# Ensure that columns exist and are correctly typed\n",
    "def ensure_int_key(df, col):\n",
    "    \"\"\"Ensure that a column exists and is converted to an integer, handling errors.\"\"\"\n",
    "    if col in df.columns:\n",
    "        try:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0).astype(int)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Error while converting {col} to int: {e}\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è Column {col} is missing in the DataFrame.\")\n",
    "    return df\n",
    "\n",
    "# Fact table creation logic\n",
    "def create_fact_sales(df_invoiceline, df_invoice, df_track, df_album, df_artist,\n",
    "                      df_customer, df_employee, df_playlisttrack, dim_location):\n",
    "    # Uppercase columns for consistency\n",
    "    df_invoiceline.columns = df_invoiceline.columns.str.upper()\n",
    "    df_invoice.columns = df_invoice.columns.str.upper()\n",
    "    df_track.columns = df_track.columns.str.upper()\n",
    "    df_album.columns = df_album.columns.str.upper()\n",
    "    df_artist.columns = df_artist.columns.str.upper()\n",
    "    df_customer.columns = df_customer.columns.str.upper()\n",
    "    df_employee.columns = df_employee.columns.str.upper()\n",
    "\n",
    "    # Rename customer billing address columns for location join\n",
    "    df_customer.rename(columns={\n",
    "        'BILLINGCITY': 'CITY',\n",
    "        'BILLINGSTATE': 'STATE',\n",
    "        'BILLINGCOUNTRY': 'COUNTRY',\n",
    "        'BILLINGPOSTALCODE': 'POSTALCODE'\n",
    "    }, inplace=True)\n",
    "\n",
    "    # Ensure key columns are integers for joining\n",
    "    for col in ['INVOICEID', 'TRACKID', 'CUSTOMERID', 'SUPPORTREPID']:\n",
    "        if col in df_invoiceline.columns:\n",
    "            df_invoiceline = ensure_int_key(df_invoiceline, col)\n",
    "    for col in ['INVOICEID', 'CUSTOMERID']:\n",
    "        if col in df_invoice.columns:\n",
    "            df_invoice = ensure_int_key(df_invoice, col)\n",
    "    if 'CUSTOMERID' in df_customer.columns:\n",
    "        df_customer = ensure_int_key(df_customer, 'CUSTOMERID')\n",
    "    if 'SUPPORTREPID' in df_customer.columns:\n",
    "        df_customer = ensure_int_key(df_customer, 'SUPPORTREPID')\n",
    "    for col in ['TRACKID', 'ALBUMID']:\n",
    "        if col in df_track.columns:\n",
    "            df_track = ensure_int_key(df_track, col)\n",
    "    for col in ['ALBUMID', 'ARTISTID']:\n",
    "        if col in df_album.columns:\n",
    "            df_album = ensure_int_key(df_album, col)\n",
    "    if 'ARTISTID' in df_artist.columns:\n",
    "        df_artist = ensure_int_key(df_artist, 'ARTISTID')\n",
    "    if 'EMPLOYEEID' in df_employee.columns:\n",
    "        df_employee = ensure_int_key(df_employee, 'EMPLOYEEID')\n",
    "\n",
    "    # Merge invoices with invoiceline and customer data\n",
    "    df_invoice.rename(columns={'INVOICEDATE': 'INVOICE_DATE', 'UNITPRICE': 'UNIT_PRICE'}, inplace=True)\n",
    "    invoice_subset = df_invoice.drop(columns=['INVOICEDATE'], errors='ignore')\n",
    "    fact = df_invoiceline.merge(invoice_subset, on='INVOICEID', how='left', suffixes=('', '_inv'))\n",
    "\n",
    "    # Resolve duplicate 'UNITPRICE'\n",
    "    if 'UNITPRICE_y' in fact.columns:\n",
    "        fact['UNIT_PRICE'] = fact['UNITPRICE_y']  # Use the merged column\n",
    "        fact = fact.drop(columns=['UNITPRICE_y'])  # Drop the duplicate\n",
    "\n",
    "    if 'UNITPRICE_x' in fact.columns:\n",
    "        fact['UNIT_PRICE'] = fact['UNITPRICE_x']  # Use the original column\n",
    "        fact = fact.drop(columns=['UNITPRICE_x'])  # Drop the duplicate\n",
    "\n",
    "    # Merge with customer and employee\n",
    "    fact = fact.merge(df_customer, on='CUSTOMERID', how='left')\n",
    "    if 'SUPPORTREPID' in df_customer.columns:\n",
    "        fact = fact.merge(df_employee, left_on='SUPPORTREPID', right_on='EMPLOYEEID', how='left')\n",
    "\n",
    "    # Merge with track, album, artist\n",
    "    fact = fact.merge(df_track, on='TRACKID', how='left')\n",
    "    fact = fact.merge(df_album, on='ALBUMID', how='left')\n",
    "    fact = fact.merge(df_artist, on='ARTISTID', how='left')\n",
    "\n",
    "    # Location merge with fallback logic\n",
    "    location_keys = ['CITY', 'STATE', 'COUNTRY', 'POSTALCODE']\n",
    "    invoice_address_keys = ['BILLINGCITY', 'BILLINGSTATE', 'BILLINGCOUNTRY', 'BILLINGPOSTALCODE']\n",
    "    if all(col in fact.columns for col in location_keys):\n",
    "        fact = fact.merge(dim_location, left_on=location_keys, right_on=location_keys, how='left')\n",
    "    elif all(col in fact.columns for col in invoice_address_keys):\n",
    "        fact = fact.merge(dim_location, left_on=invoice_address_keys, right_on=location_keys, how='left')\n",
    "\n",
    "    # Calculate EXTENDED_PRICE\n",
    "    if 'UNIT_PRICE' in fact.columns:\n",
    "        fact['EXTENDED_PRICE'] = fact['UNIT_PRICE'] * fact['QUANTITY']\n",
    "\n",
    "    # Rename columns for fact table output\n",
    "    rename_map = {\n",
    "        'INVOICELINEID': 'INVOICELINE_ID',\n",
    "        'INVOICEID': 'INVOICE_ID',\n",
    "        'INVOICE_DATE': 'INVOICE_DATE',\n",
    "        'TRACKID': 'TRACK_ID',\n",
    "        'CUSTOMERID': 'CUSTOMER_ID',\n",
    "        'UNIT_PRICE': 'UNIT_PRICE',\n",
    "        'QUANTITY': 'QUANTITY',\n",
    "        'EXTENDED_PRICE': 'EXTENDED_PRICE'\n",
    "    }\n",
    "    if 'SUPPORTREPID' in fact.columns:\n",
    "        rename_map['SUPPORTREPID'] = 'EMPLOYEE_ID'\n",
    "\n",
    "    fact_sales = fact.rename(columns=rename_map)\n",
    "\n",
    "    # Keep only relevant columns in fact_sales\n",
    "    columns_to_keep = [\n",
    "        'INVOICELINE_ID', 'LOCATION_ID', 'INVOICE_ID', 'TRACK_ID', 'CUSTOMER_ID',\n",
    "        'EMPLOYEE_ID', 'MEDIATYPE_ID', 'INVOICE_DATE', 'UNIT_PRICE', 'QUANTITY',\n",
    "        'TOTAL_AMOUNT', 'PLAYLIST_ID', 'ALBUM_ID', 'ARTIST_ID', 'MILLISECONDS', 'BYTES'\n",
    "    ]\n",
    "    fact_sales = fact_sales[columns_to_keep]\n",
    "\n",
    "    return fact_sales.reset_index(drop=True)\n",
    "\n",
    "# Proceed with further transformations, save CSVs in .csv.gz format, and load into Snowflake stage\n",
    "\n",
    "# Save transformed dataframes as CSV.gz files\n",
    "fact_sales.to_csv('Fact_Sales_star.csv.gz', index=False, compression='gzip')\n",
    "dim_location.to_csv('Dim_Location_star.csv.gz', index=False, compression='gzip')\n",
    "dim_date.to_csv('Dim_Date_star.csv.gz', index=False, compression='gzip')\n",
    "dim_album_artist.to_csv('Dim_Album_Artist_star.csv.gz', index=False, compression='gzip')\n",
    "dim_track.to_csv('Dim_Track_star.csv.gz', index=False, compression='gzip')\n",
    "dim_playlist_track.to_csv('Dim_Playlist_Track_star.csv.gz', index=False, compression='gzip')\n",
    "dim_employee.to_csv('Dim_Employee_star.csv.gz', index=False, compression='gzip')\n",
    "dim_customer.to_csv('Dim_Customer_star.csv.gz', index=False, compression='gzip')\n",
    "\n",
    "# Upload CSVs to Snowflake stage FINAL_TRANSFORMED_STAGE (upload manually via Snowflake UI or use PUT command)\n",
    "dim_date = create_dim_date(df_invoice)\n",
    "dim_location = create_dim_location(df_customer)\n",
    "dim_album_artist = create_dim_album_artist(df_album, df_artist)\n",
    "dim_track = create_dim_track(df_track, df_genre, df_mediatype)\n",
    "dim_playlist_track = create_dim_playlist_track(df_playlisttrack, df_playlist)\n",
    "dim_employee = create_dim_employee(df_employee)\n",
    "dim_customer = create_dim_customer(df_customer)\n",
    "\n",
    "fact_sales = create_fact_sales(df_invoiceline, df_invoice, df_track, df_album, df_artist,\n",
    "                               df_customer, df_employee, df_playlisttrack, dim_location)\n",
    "\n",
    "# Switch to final schema\n",
    "set_schema(final_schema)\n",
    "\n",
    "def load_df_to_table(df, table_name):\n",
    "    print(f\"\\nLoading {table_name}...\")\n",
    "    print(f\"{table_name} rows: {len(df)}\")\n",
    "    print(df.head())\n",
    "    # Data loading is performed using the Snowflake stage after uploading CSVs\n",
    "    session.sql(\"\"\"\n",
    "        COPY INTO ERD_SCHEMA_STAR.DIM_DATE\n",
    "        FROM @FINAL_TRANSFORMED_STAGE/Dim_Date_star.csv.gz\n",
    "        FILE_FORMAT = (TYPE = 'CSV' COMPRESSION = 'GZIP');\n",
    "    \"\"\").collect()\n",
    "\n",
    "    session.sql(\"\"\"\n",
    "        COPY INTO ERD_SCHEMA_STAR.DIM_LOCATION\n",
    "        FROM @FINAL_TRANSFORMED_STAGE/Dim_Location_star.csv.gz\n",
    "        FILE_FORMAT = (TYPE = 'CSV' COMPRESSION = 'GZIP');\n",
    "    \"\"\").collect()\n",
    "\n",
    "    session.sql(\"\"\"\n",
    "        COPY INTO ERD_SCHEMA_STAR.DIM_CUSTOMER\n",
    "        FROM @FINAL_TRANSFORMED_STAGE/Dim_Customer_star.csv.gz\n",
    "        FILE_FORMAT = (TYPE = 'CSV' COMPRESSION = 'GZIP');\n",
    "    \"\"\").collect()\n",
    "\n",
    "    session.sql(\"\"\"\n",
    "        COPY INTO ERD_SCHEMA_STAR.FACTSALES\n",
    "        FROM @FINAL_TRANSFORMED_STAGE/Fact_Sales_star.csv.gz\n",
    "        FILE_FORMAT = (TYPE = 'CSV' COMPRESSION = 'GZIP');\n",
    "    \"\"\").collect()\n",
    "\n",
    "# Calling the function to execute the COPY commands\n",
    "load_df_to_table(dim_date, \"DIM_DATE\")\n",
    "load_df_to_table(dim_location, \"DIM_LOCATION\")\n",
    "load_df_to_table(dim_album_artist, \"DIM_ALBUM_ARTIST\")\n",
    "load_df_to_table(dim_track, \"DIM_TRACK\")\n",
    "load_df_to_table(dim_playlist_track, \"DIM_PLAYLIST_TRACK\")\n",
    "load_df_to_table(dim_employee, \"DIM_EMPLOYEE\")\n",
    "load_df_to_table(dim_customer, \"DIM_CUSTOMER\")\n",
    "load_df_to_table(fact_sales, \"FACTSALES\")\n",
    "\n",
    "print(\"‚úÖ All dimensions and facts loaded into ERD_SCHEMA_STAR successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ee7c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Switch to final schema\n",
    " #Calling the function to execute the COPY commands\n",
    "load_df_to_table(dim_date, \"DIM_DATE\")\n",
    "load_df_to_table(dim_location, \"DIM_LOCATION\")\n",
    "load_df_to_table(dim_album_artist, \"DIM_ALBUM_ARTIST\")\n",
    "load_df_to_table(dim_track, \"DIM_TRACK\")\n",
    "load_df_to_table(dim_playlist_track, \"DIM_PLAYLIST_TRACK\")\n",
    "load_df_to_table(dim_employee, \"DIM_EMPLOYEE\")\n",
    "load_df_to_table(dim_customer, \"DIM_CUSTOMER\")\n",
    "load_df_to_table(fact_sales, \"FACTSALES\")\n",
    "\n",
    "set_schema(final_schema)\n",
    "# Calling the function to execute the COPY commands\n",
    "load_df_to_table(dim_date, \"DIM_DATE\")\n",
    "load_df_to_table(dim_location, \"DIM_LOCATION\")\n",
    "load_df_to_table(dim_album_artist, \"DIM_ALBUM_ARTIST\")\n",
    "load_df_to_table(dim_track, \"DIM_TRACK\")\n",
    "load_df_to_table(dim_playlist_track, \"DIM_PLAYLIST_TRACK\")\n",
    "load_df_to_table(dim_employee, \"DIM_EMPLOYEE\")\n",
    "load_df_to_table(dim_customer, \"DIM_CUSTOMER\")\n",
    "load_df_to_table(fact_sales, \"FACTSALES\")\n",
    "\n",
    "\n",
    "def load_df_to_table(df, table_name):\n",
    "    print(f\"\\nLoading {table_name}...\")\n",
    "    print(f\"{table_name} rows: {len(df)}\")\n",
    "    print(df.head())\n",
    "    # Data loading is performed using the Snowflake stage after uploading CSVs\n",
    "    # Replace the session.write_pandas part with COPY INTO commands to load data from the stage into the final tables\n",
    "    session.sql(\"\"\"\n",
    "    COPY INTO ERD_SCHEMA_STAR.DIM_DATE\n",
    "    FROM @FINAL_TRANSFORMED_STAGE/Dim_Date_star.csv.gz\n",
    "    FILE_FORMAT = (TYPE = 'CSV' COMPRESSION = 'GZIP');\n",
    "    \"\"\").collect()\n",
    "\n",
    "    session.sql(\"\"\"\n",
    "    COPY INTO ERD_SCHEMA_STAR.DIM_LOCATION\n",
    "    FROM @FINAL_TRANSFORMED_STAGE/Dim_Location_star.csv.gz\n",
    "    FILE_FORMAT = (TYPE = 'CSV' COMPRESSION = 'GZIP');\n",
    "    \"\"\").collect()\n",
    "\n",
    "    session.sql(\"\"\"\n",
    "    COPY INTO ERD_SCHEMA_STAR.DIM_CUSTOMER\n",
    "    FROM @FINAL_TRANSFORMED_STAGE/Dim_Customer_star.csv.gz\n",
    "    FILE_FORMAT = (TYPE = 'CSV' COMPRESSION = 'GZIP');\n",
    "    \"\"\").collect()\n",
    "\n",
    "    session.sql(\"\"\"\n",
    "    COPY INTO ERD_SCHEMA_STAR.FACTSALES\n",
    "    FROM @FINAL_TRANSFORMED_STAGE/Fact_Sales_star.csv.gz\n",
    "    FILE_FORMAT = (TYPE = 'CSV' COMPRESSION = 'GZIP');\n",
    "    \"\"\").collect()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "snowflake",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
