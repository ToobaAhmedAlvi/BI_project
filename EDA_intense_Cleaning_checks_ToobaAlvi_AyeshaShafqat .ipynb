{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "979187c9-2a5d-4d21-bd2b-14a8ad152123",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "2998bd08-9ac0-47af-9e2a-5bc60561f532",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Album loaded successfully. Shape: (347, 3)\n",
      "✅ Album_Errors loaded successfully. Shape: (352, 3)\n",
      "✅ Artist loaded successfully. Shape: (275, 2)\n",
      "✅ Artist_Errors loaded successfully. Shape: (279, 2)\n",
      "✅ Customer loaded successfully. Shape: (59, 13)\n",
      "✅ Customer_Errors loaded successfully. Shape: (59, 13)\n",
      "✅ Employee loaded successfully. Shape: (8, 15)\n",
      "✅ Genre loaded successfully. Shape: (25, 2)\n",
      "✅ Invoice loaded successfully. Shape: (412, 9)\n",
      "✅ InvoiceLine loaded successfully. Shape: (2240, 5)\n",
      "✅ MediaType loaded successfully. Shape: (5, 2)\n",
      "✅ Playlist loaded successfully. Shape: (18, 2)\n",
      "✅ PlaylistTrack loaded successfully. Shape: (8715, 2)\n",
      "✅ Track loaded successfully. Shape: (3503, 9)\n",
      "\n",
      "✅ All datasets loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# Importing datasets \n",
    "dataset_path = r\"C:\\Users\\ayesh\\Desktop\\DWH Project\"\n",
    "\n",
    "files = {\n",
    "    \"Album\": \"Album.csv\",\n",
    "    \"Album_Errors\": \"Album_with_errors.csv\",\n",
    "    \"Artist\": \"Artist.csv\",\n",
    "    \"Artist_Errors\": \"Artist_with_errors.csv\",\n",
    "    \"Customer\": \"Customer.csv\",\n",
    "    \"Customer_Errors\": \"Customer_with_errors.csv\",\n",
    "    \"Employee\": \"Employee.csv\",\n",
    "    \"Genre\": \"Genre.csv\",\n",
    "    \"Invoice\": \"Invoice.csv\",\n",
    "    \"InvoiceLine\": \"InvoiceLine.csv\",\n",
    "    \"MediaType\": \"MediaType.csv\",\n",
    "    \"Playlist\": \"Playlist.csv\",\n",
    "    \"PlaylistTrack\": \"PlaylistTrack.csv\",\n",
    "    \"Track\": \"Track.csv\"\n",
    "}\n",
    "\n",
    "# Load all datasets\n",
    "dataframes = {}\n",
    "for name, filename in files.items():\n",
    "    file_path = os.path.join(dataset_path, filename)\n",
    "    if os.path.isfile(file_path):\n",
    "        try:\n",
    "            df = pd.read_csv(file_path)\n",
    "            dataframes[name] = df\n",
    "            print(f\"✅ {name} loaded successfully. Shape: {df.shape}\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error loading {name}: {e}\")\n",
    "    else:\n",
    "        print(f\"❌ File not found: {file_path}\")\n",
    "\n",
    "print(\"\\n✅ All datasets loaded successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "a8e99ac2-2a69-4276-8bb6-870a4bedf4c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Starting Data Type Audit ===\n",
      "\n",
      "Checking dataset: Album\n",
      "\n",
      "Checking dataset: Album_Errors\n",
      "\n",
      "Checking dataset: Artist\n",
      "\n",
      "Checking dataset: Artist_Errors\n",
      "\n",
      "Checking dataset: Customer\n",
      "\n",
      "Checking dataset: Customer_Errors\n",
      "\n",
      "Checking dataset: Employee\n",
      "\n",
      "Checking dataset: Genre\n",
      "\n",
      "Checking dataset: Invoice\n",
      "\n",
      "Checking dataset: InvoiceLine\n",
      "\n",
      "Checking dataset: MediaType\n",
      "\n",
      "Checking dataset: Playlist\n",
      "\n",
      "Checking dataset: PlaylistTrack\n",
      "\n",
      "Checking dataset: Track\n",
      "\n",
      "=== Data Type Audit Completed ===\n"
     ]
    }
   ],
   "source": [
    "# Expected schema \n",
    "expected_types = {\n",
    "    'Album': {'AlbumId': 'Int64', 'Title': 'object', 'ArtistId': 'Int64'},\n",
    "    'Album_Errors': {'AlbumId': 'Int64', 'Title': 'object', 'ArtistId': 'Int64'},\n",
    "    'Artist': {'ArtistId': 'Int64', 'Name': 'object'},\n",
    "    'Artist_Errors': {'ArtistId': 'Int64', 'Name': 'object'},\n",
    "    'Customer': {\n",
    "        'CustomerId': 'Int64', 'FirstName': 'object', 'LastName': 'object', 'Company': 'object',\n",
    "        'Address': 'object', 'City': 'object', 'State': 'object', 'Country': 'object',\n",
    "        'PostalCode': 'object', 'Phone': 'object', 'Fax': 'object', 'Email': 'object', 'SupportRepId': 'Int64'\n",
    "    },\n",
    "    'Customer_Errors': {\n",
    "        'CustomerId': 'Int64', 'FirstName': 'object', 'LastName': 'object', 'Company': 'object',\n",
    "        'Address': 'object', 'City': 'object', 'State': 'object', 'Country': 'object',\n",
    "        'PostalCode': 'object', 'Phone': 'object', 'Fax': 'object', 'Email': 'object', 'SupportRepId': 'Int64'\n",
    "    },\n",
    "    'Employee': {\n",
    "        'EmployeeId': 'Int64', 'LastName': 'object', 'FirstName': 'object', 'Title': 'object',\n",
    "        'ReportsTo': 'Int64', 'BirthDate': 'datetime64[ns]', 'HireDate': 'datetime64[ns]',\n",
    "        'Address': 'object', 'City': 'object', 'State': 'object', 'Country': 'object',\n",
    "        'PostalCode': 'object', 'Phone': 'object', 'Fax': 'object', 'Email': 'object'\n",
    "    },\n",
    "    'Genre': {'GenreId': 'Int64', 'Name': 'object'},\n",
    "    'Invoice': {\n",
    "        'InvoiceId': 'Int64', 'CustomerId': 'Int64', 'InvoiceDate': 'datetime64[ns]',\n",
    "        'BillingAddress': 'object', 'BillingCity': 'object', 'BillingState': 'object',\n",
    "        'BillingCountry': 'object', 'BillingPostalCode': 'object', 'Total': 'float64'\n",
    "    },\n",
    "    'InvoiceLine': {\n",
    "        'InvoiceLineId': 'Int64', 'InvoiceId': 'Int64', 'TrackId': 'Int64', 'UnitPrice': 'float64', 'Quantity': 'Int64'\n",
    "    },\n",
    "    'MediaType': {'MediaTypeId': 'Int64', 'Name': 'object'},\n",
    "    'Playlist': {'PlaylistId': 'Int64', 'Name': 'object'},\n",
    "    'PlaylistTrack': {'PlaylistId': 'Int64', 'TrackId': 'Int64'},\n",
    "    'Track': {\n",
    "        'TrackId': 'Int64', 'Name': 'object', 'AlbumId': 'Int64', 'MediaTypeId': 'Int64',\n",
    "        'GenreId': 'Int64', 'Composer': 'object', 'Milliseconds': 'Int64', 'Bytes': 'Int64', 'UnitPrice': 'float64'\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"=== Starting Data Type Audit ===\")\n",
    "for ds_name, df in dataframes.items():\n",
    "    print(f\"\\nChecking dataset: {ds_name}\")\n",
    "    if ds_name not in expected_types:\n",
    "        print(\"  No schema info available. Skipping.\")\n",
    "        continue\n",
    "    schema = expected_types[ds_name]\n",
    "    for col, expected_dtype in schema.items():\n",
    "        if col not in df.columns:\n",
    "            print(f\"  ❌ Missing expected column: {col}\")\n",
    "            continue\n",
    "        actual_dtype = str(df[col].dtype)\n",
    "        if expected_dtype == 'datetime64[ns]':\n",
    "            # Check convertibility to datetime\n",
    "            try:\n",
    "                pd.to_datetime(df[col], errors='raise')\n",
    "            except:\n",
    "                print(f\"  ⚠️ Column '{col}' NOT convertible to datetime.\")\n",
    "        elif actual_dtype != expected_dtype:\n",
    "            print(f\"  ⚠️ Column '{col}' type mismatch: expected {expected_dtype}, found {actual_dtype}.\")\n",
    "print(\"\\n=== Data Type Audit Completed ===\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "707895c8-fb5b-42cf-ac08-b2e2a2ace5bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixing data types for Album...\n",
      "Fixing data types for Album_Errors...\n",
      "Fixing data types for Artist...\n",
      "Fixing data types for Artist_Errors...\n",
      "Fixing data types for Customer...\n",
      "Fixing data types for Customer_Errors...\n",
      "Fixing data types for Employee...\n",
      "Fixing data types for Genre...\n",
      "Fixing data types for Invoice...\n",
      "Fixing data types for InvoiceLine...\n",
      "Fixing data types for MediaType...\n",
      "Fixing data types for Playlist...\n",
      "Fixing data types for PlaylistTrack...\n",
      "Fixing data types for Track...\n",
      "Data types fixed where needed.\n"
     ]
    }
   ],
   "source": [
    "def fix_data_types(df, schema):\n",
    "    for col, dtype in schema.items():\n",
    "        if col not in df.columns:\n",
    "            continue\n",
    "        try:\n",
    "            if dtype == 'Int64':\n",
    "                df[col] = pd.to_numeric(df[col], errors='coerce').astype('Int64')\n",
    "            elif dtype == 'float64':\n",
    "                df[col] = pd.to_numeric(df[col], errors='coerce').astype('float64')\n",
    "            elif dtype == 'datetime64[ns]':\n",
    "                df[col] = pd.to_datetime(df[col], errors='coerce')\n",
    "            else:\n",
    "                df[col] = df[col].astype(dtype)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to convert {col} in {df.name if hasattr(df, 'name') else 'DataFrame'}: {e}\")\n",
    "    return df\n",
    "\n",
    "for ds_name, df in dataframes.items():\n",
    "    if ds_name in expected_types:\n",
    "        print(f\"Fixing data types for {ds_name}...\")\n",
    "        dataframes[ds_name] = fix_data_types(df, expected_types[ds_name])\n",
    "print(\"Data types fixed where needed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "107d4070-27f9-432f-90b5-a7c3d27c939f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Missing Values Audit ===\n",
      "\n",
      "Dataset: Album\n",
      "  No missing values detected.\n",
      "\n",
      "Dataset: Album_Errors\n",
      "AlbumId     5\n",
      "Title       1\n",
      "ArtistId    4\n",
      "dtype: int64\n",
      "\n",
      "Dataset: Artist\n",
      "  No missing values detected.\n",
      "\n",
      "Dataset: Artist_Errors\n",
      "ArtistId    1\n",
      "Name        5\n",
      "dtype: int64\n",
      "\n",
      "Dataset: Customer\n",
      "Company       49\n",
      "State         29\n",
      "PostalCode     4\n",
      "Phone          1\n",
      "Fax           47\n",
      "dtype: int64\n",
      "\n",
      "Dataset: Customer_Errors\n",
      "Company       49\n",
      "State         29\n",
      "PostalCode     4\n",
      "Phone          1\n",
      "Fax           47\n",
      "dtype: int64\n",
      "\n",
      "Dataset: Employee\n",
      "ReportsTo    1\n",
      "dtype: int64\n",
      "\n",
      "Dataset: Genre\n",
      "  No missing values detected.\n",
      "\n",
      "Dataset: Invoice\n",
      "BillingState         202\n",
      "BillingPostalCode     28\n",
      "dtype: int64\n",
      "\n",
      "Dataset: InvoiceLine\n",
      "  No missing values detected.\n",
      "\n",
      "Dataset: MediaType\n",
      "  No missing values detected.\n",
      "\n",
      "Dataset: Playlist\n",
      "  No missing values detected.\n",
      "\n",
      "Dataset: PlaylistTrack\n",
      "  No missing values detected.\n",
      "\n",
      "Dataset: Track\n",
      "Composer    978\n",
      "dtype: int64\n",
      "\n",
      "=== Missing Values Audit Completed ===\n"
     ]
    }
   ],
   "source": [
    "#Missing values\n",
    "\n",
    "print(\"=== Missing Values Audit ===\")\n",
    "for ds_name, df in dataframes.items():\n",
    "    print(f\"\\nDataset: {ds_name}\")\n",
    "    missing_counts = df.isnull().sum()\n",
    "    if missing_counts.sum() == 0:\n",
    "        print(\"  No missing values detected.\")\n",
    "    else:\n",
    "        print(missing_counts[missing_counts > 0])\n",
    "print(\"\\n=== Missing Values Audit Completed ===\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "af241dab-3ec6-41cf-927f-2ac1a9ea8cae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixing missing values in Album...\n",
      "Fixing missing values in Album_Errors...\n",
      "Fixing missing values in Artist...\n",
      "Fixing missing values in Artist_Errors...\n",
      "Fixing missing values in Customer...\n",
      "Fixing missing values in Customer_Errors...\n",
      "Fixing missing values in Employee...\n",
      "Fixing missing values in Genre...\n",
      "Fixing missing values in Invoice...\n",
      "Fixing missing values in InvoiceLine...\n",
      "Fixing missing values in MediaType...\n",
      "Fixing missing values in Playlist...\n",
      "Fixing missing values in PlaylistTrack...\n",
      "Fixing missing values in Track...\n",
      "Missing values fixed.\n"
     ]
    }
   ],
   "source": [
    "#Fix missing values where critical (PK/FK columns), fill others\n",
    "\n",
    "def fix_missing_values(df):\n",
    "    # Drop rows where any PK or FK columns are missing\n",
    "    pk_fk_cols = [col for col in df.columns if 'Id' in col or 'Key' in col]\n",
    "    df.dropna(subset=pk_fk_cols, inplace=True)\n",
    "    # For non-key columns, fill missing strings with 'Unknown' and numbers with 0\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == 'object':\n",
    "            df[col] = df[col].fillna('Unknown')\n",
    "        elif pd.api.types.is_numeric_dtype(df[col]):\n",
    "            df[col] = df[col].fillna(0)\n",
    "        else:\n",
    "            # For datetime or other types, leave NaNs for now\n",
    "            pass\n",
    "    return df\n",
    "\n",
    "for ds_name, df in dataframes.items():\n",
    "    print(f\"Fixing missing values in {ds_name}...\")\n",
    "    dataframes[ds_name] = fix_missing_values(df)\n",
    "print(\"Missing values fixed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "5e5980f8-34d6-4455-a828-d355aa4d94b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Duplicate Primary Key Audit ===\n",
      "Album - Duplicates in AlbumId: 0\n",
      "Album_Errors - Duplicates in AlbumId: 5\n",
      "Artist - Duplicates in ArtistId: 0\n",
      "Artist_Errors - Duplicates in ArtistId: 5\n",
      "Customer - Duplicates in CustomerId: 0\n",
      "Customer_Errors - Duplicates in CustomerId: 0\n",
      "Employee - Duplicates in EmployeeId: 0\n",
      "Genre - Duplicates in GenreId: 0\n",
      "Invoice - Duplicates in InvoiceId: 0\n",
      "InvoiceLine - Duplicates in InvoiceLineId: 0\n",
      "MediaType - Duplicates in MediaTypeId: 0\n",
      "Playlist - Duplicates in PlaylistId: 0\n",
      "PlaylistTrack: Skipping composite or no single PK.\n",
      "Track - Duplicates in TrackId: 0\n",
      "=== Duplicate Audit Completed ===\n"
     ]
    }
   ],
   "source": [
    "#Duplicates\n",
    "primary_key_cols = {\n",
    "    \"Album\": \"AlbumId\",\n",
    "    \"Album_Errors\": \"AlbumId\",\n",
    "    \"Artist\": \"ArtistId\",\n",
    "    \"Artist_Errors\": \"ArtistId\",\n",
    "    \"Customer\": \"CustomerId\",\n",
    "    \"Customer_Errors\": \"CustomerId\",\n",
    "    \"Employee\": \"EmployeeId\",\n",
    "    \"Genre\": \"GenreId\",\n",
    "    \"Invoice\": \"InvoiceId\",\n",
    "    \"InvoiceLine\": \"InvoiceLineId\",\n",
    "    \"MediaType\": \"MediaTypeId\",\n",
    "    \"Playlist\": \"PlaylistId\",\n",
    "    \"PlaylistTrack\": None,  # composite key, no single PK\n",
    "    \"Track\": \"TrackId\"\n",
    "}\n",
    "\n",
    "print(\"=== Duplicate Primary Key Audit ===\")\n",
    "for ds_name, pk_col in primary_key_cols.items():\n",
    "    if pk_col is None:\n",
    "        print(f\"{ds_name}: Skipping composite or no single PK.\")\n",
    "        continue\n",
    "    df = dataframes[ds_name]\n",
    "    dup_count = df[pk_col].duplicated().sum()\n",
    "    print(f\"{ds_name} - Duplicates in {pk_col}: {dup_count}\")\n",
    "print(\"=== Duplicate Audit Completed ===\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "8a108fe3-4f98-4206-afb7-63e747e45411",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing duplicates in Album based on AlbumId...\n",
      "Removing duplicates in Album_Errors based on AlbumId...\n",
      "Removing duplicates in Artist based on ArtistId...\n",
      "Removing duplicates in Artist_Errors based on ArtistId...\n",
      "Removing duplicates in Customer based on CustomerId...\n",
      "Removing duplicates in Customer_Errors based on CustomerId...\n",
      "Removing duplicates in Employee based on EmployeeId...\n",
      "Removing duplicates in Genre based on GenreId...\n",
      "Removing duplicates in Invoice based on InvoiceId...\n",
      "Removing duplicates in InvoiceLine based on InvoiceLineId...\n",
      "Removing duplicates in MediaType based on MediaTypeId...\n",
      "Removing duplicates in Playlist based on PlaylistId...\n",
      "Removing duplicates in Track based on TrackId...\n",
      "Duplicate primary keys removed.\n"
     ]
    }
   ],
   "source": [
    "# Fix duplicates\n",
    "\n",
    "for ds_name, pk_col in primary_key_cols.items():\n",
    "    if pk_col is None:\n",
    "        continue\n",
    "    df = dataframes[ds_name]\n",
    "    print(f\"Removing duplicates in {ds_name} based on {pk_col}...\")\n",
    "    dataframes[ds_name] = df.drop_duplicates(subset=[pk_col], keep='first')\n",
    "print(\"Duplicate primary keys removed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "f0801c39-d55b-4995-8421-b7c43ab3a806",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Foreign Key Integrity Audit ===\n",
      "Album referencing Artist on ArtistId: 0 invalid rows\n",
      "Track referencing Genre on GenreId: 0 invalid rows\n",
      "Invoice referencing Customer on CustomerId: 0 invalid rows\n",
      "InvoiceLine referencing Track on TrackId: 0 invalid rows\n",
      "PlaylistTrack referencing Track on TrackId: 0 invalid rows\n",
      "=== Foreign Key Audit Completed ===\n"
     ]
    }
   ],
   "source": [
    "#Audit foreign key integrity\n",
    "\n",
    "fk_relations = {\n",
    "    \"Album\": (\"ArtistId\", \"Artist\", \"ArtistId\"),\n",
    "    \"Track\": (\"AlbumId\", \"Album\", \"AlbumId\"),\n",
    "    \"Invoice\": (\"CustomerId\", \"Customer\", \"CustomerId\"),\n",
    "    \"InvoiceLine\": (\"InvoiceId\", \"Invoice\", \"InvoiceId\"),\n",
    "    \"InvoiceLine\": (\"TrackId\", \"Track\", \"TrackId\"),\n",
    "    \"Track\": (\"MediaTypeId\", \"MediaType\", \"MediaTypeId\"),\n",
    "    \"Track\": (\"GenreId\", \"Genre\", \"GenreId\"),\n",
    "    \"PlaylistTrack\": (\"PlaylistId\", \"Playlist\", \"PlaylistId\"),\n",
    "    \"PlaylistTrack\": (\"TrackId\", \"Track\", \"TrackId\")\n",
    "}\n",
    "\n",
    "print(\"=== Foreign Key Integrity Audit ===\")\n",
    "for child, (fk_col, parent, pk_col) in fk_relations.items():\n",
    "    child_df = dataframes[child]\n",
    "    parent_df = dataframes[parent]\n",
    "    invalid_fk = child_df[~child_df[fk_col].isin(parent_df[pk_col])]\n",
    "    print(f\"{child} referencing {parent} on {fk_col}: {len(invalid_fk)} invalid rows\")\n",
    "print(\"=== Foreign Key Audit Completed ===\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "a83bd482-0f6f-4528-b2ba-c9a14509407c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing invalid foreign keys in Album referencing Artist on ArtistId...\n",
      "Removing invalid foreign keys in Track referencing Genre on GenreId...\n",
      "Removing invalid foreign keys in Invoice referencing Customer on CustomerId...\n",
      "Removing invalid foreign keys in InvoiceLine referencing Track on TrackId...\n",
      "Removing invalid foreign keys in PlaylistTrack referencing Track on TrackId...\n",
      "Foreign key integrity fixed.\n"
     ]
    }
   ],
   "source": [
    "#Fixing foreign key integrity by removing invalid row\n",
    "\n",
    "for child, (fk_col, parent, pk_col) in fk_relations.items():\n",
    "    child_df = dataframes[child]\n",
    "    parent_df = dataframes[parent]\n",
    "    print(f\"Removing invalid foreign keys in {child} referencing {parent} on {fk_col}...\")\n",
    "    dataframes[child] = child_df[child_df[fk_col].isin(parent_df[pk_col])]\n",
    "print(\"Foreign key integrity fixed.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "48fdc075-2860-4e7e-80f0-66d3a6d0cb05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Date Format Audit ===\n",
      "Employee - BirthDate invalid dates count: 0\n",
      "Employee - HireDate invalid dates count: 0\n",
      "Invoice - InvoiceDate invalid dates count: 0\n",
      "=== Date Audit Completed ===\n"
     ]
    }
   ],
   "source": [
    "#Date Format Check\n",
    "\n",
    "date_columns = {\n",
    "    \"Employee\": [\"BirthDate\", \"HireDate\"],\n",
    "    \"Invoice\": [\"InvoiceDate\"]\n",
    "}\n",
    "\n",
    "print(\"=== Date Format Audit ===\")\n",
    "for ds_name, cols in date_columns.items():\n",
    "    df = dataframes[ds_name]\n",
    "    for col in cols:\n",
    "        invalid_dates = pd.to_datetime(df[col], errors='coerce').isna()\n",
    "        count_invalid = invalid_dates.sum()\n",
    "        print(f\"{ds_name} - {col} invalid dates count: {count_invalid}\")\n",
    "print(\"=== Date Audit Completed ===\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "03d3be3e-2846-4697-a487-2e6bc9863cd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date columns fixed.\n"
     ]
    }
   ],
   "source": [
    "#Basic outlier check for key numeric columns\n",
    "\n",
    "for ds_name, cols in date_columns.items():\n",
    "    df = dataframes[ds_name]\n",
    "    for col in cols:\n",
    "        df[col] = pd.to_datetime(df[col], errors='coerce')\n",
    "        df = df.dropna(subset=[col])\n",
    "    dataframes[ds_name] = df\n",
    "print(\"Date columns fixed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "35ac04a0-ef3f-4423-a125-786bec51cd1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Numeric Outlier Audit (Basic) ===\n",
      "InvoiceLine - UnitPrice negative values count: 0\n",
      "InvoiceLine - Quantity negative values count: 0\n",
      "Track - Milliseconds negative values count: 0\n",
      "Track - Bytes negative values count: 0\n",
      "Track - UnitPrice negative values count: 0\n",
      "Invoice - Total negative values count: 0\n",
      "=== Numeric Outlier Audit Completed ===\n"
     ]
    }
   ],
   "source": [
    "#Basic outlier check for key numeric columns\n",
    "\n",
    "numeric_checks = {\n",
    "    \"InvoiceLine\": [\"UnitPrice\", \"Quantity\"],\n",
    "    \"Track\": [\"Milliseconds\", \"Bytes\", \"UnitPrice\"],\n",
    "    \"Invoice\": [\"Total\"]\n",
    "}\n",
    "\n",
    "print(\"=== Numeric Outlier Audit (Basic) ===\")\n",
    "for ds_name, cols in numeric_checks.items():\n",
    "    df = dataframes[ds_name]\n",
    "    for col in cols:\n",
    "        if col in df.columns:\n",
    "            invalid_neg = (df[col] < 0).sum()\n",
    "            print(f\"{ds_name} - {col} negative values count: {invalid_neg}\")\n",
    "print(\"=== Numeric Outlier Audit Completed ===\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "0a486250-6c7d-4557-bac8-c12b2f58d625",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative numeric values removed.\n"
     ]
    }
   ],
   "source": [
    "#Fix by removing negative numeric values\n",
    "\n",
    "for ds_name, cols in numeric_checks.items():\n",
    "    df = dataframes[ds_name]\n",
    "    for col in cols:\n",
    "        if col in df.columns:\n",
    "            df = df[df[col] >= 0]\n",
    "    dataframes[ds_name] = df\n",
    "print(\"Negative numeric values removed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "1d4e19a7-6297-4b21-a02f-35feb784313d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Email Format Audit ===\n",
      "Customer: 0 invalid emails\n",
      "Customer_Errors: 0 invalid emails\n",
      "Employee: 0 invalid emails\n",
      "=== Email Audit Completed ===\n"
     ]
    }
   ],
   "source": [
    "#Checking email format validity (Emails, Phones, Postal Codes)\n",
    "\n",
    "import re\n",
    "\n",
    "def check_email_format(df, col='Email'):\n",
    "    if col not in df.columns:\n",
    "        return 0\n",
    "    invalid_emails = df[~df[col].astype(str).str.match(r'^[\\w\\.-]+@[\\w\\.-]+\\.\\w{2,4}$', na=False)]\n",
    "    return len(invalid_emails)\n",
    "\n",
    "print(\"=== Email Format Audit ===\")\n",
    "for ds_name in ['Customer', 'Customer_Errors', 'Employee']:\n",
    "    df = dataframes[ds_name]\n",
    "    invalid_count = check_email_format(df)\n",
    "    print(f\"{ds_name}: {invalid_count} invalid emails\")\n",
    "print(\"=== Email Audit Completed ===\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "edb2d178-0db7-4a7f-8263-8f4b3a8933f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emails fixed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ayesh\\AppData\\Local\\Temp\\ipykernel_2340\\4220622659.py:7: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna('Unknown', inplace=True)\n",
      "C:\\Users\\ayesh\\AppData\\Local\\Temp\\ipykernel_2340\\4220622659.py:7: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna('Unknown', inplace=True)\n",
      "C:\\Users\\ayesh\\AppData\\Local\\Temp\\ipykernel_2340\\4220622659.py:7: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna('Unknown', inplace=True)\n"
     ]
    }
   ],
   "source": [
    "#Fixing emails by replacing invalids with NaN then fillna\n",
    "\n",
    "def fix_email_format(df, col='Email'):\n",
    "    if col not in df.columns:\n",
    "        return df\n",
    "    df.loc[~df[col].astype(str).str.match(r'^[\\w\\.-]+@[\\w\\.-]+\\.\\w{2,4}$', na=False), col] = np.nan\n",
    "    df[col].fillna('Unknown', inplace=True)\n",
    "    return df\n",
    "\n",
    "for ds_name in ['Customer', 'Customer_Errors', 'Employee']:\n",
    "    df = dataframes[ds_name]\n",
    "    dataframes[ds_name] = fix_email_format(df)\n",
    "print(\"Emails fixed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "921ec607-ea72-4726-b80d-b5d17f90d7aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning phones and postals in Customer...\n",
      "Cleaning phones and postals in Employee...\n",
      "Cleaning phones and postals in Invoice...\n",
      "Phones and postal codes cleaned.\n"
     ]
    }
   ],
   "source": [
    "#Simple cleaning (remove non-numeric for phones, trim, fill unknown)\n",
    "\n",
    "def clean_phone_postal(df):\n",
    "    phone_cols = ['Phone', 'Fax']\n",
    "    postal_cols = ['PostalCode', 'BillingPostalCode']\n",
    "    for col in phone_cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].astype(str).str.replace(r'\\D+', '', regex=True).replace({'': 'Unknown'})\n",
    "    for col in postal_cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].astype(str).str.strip().replace({'': 'Unknown'})\n",
    "    return df\n",
    "\n",
    "for ds_name in ['Customer', 'Employee', 'Invoice']:\n",
    "    df = dataframes.get(ds_name)\n",
    "    if df is not None:\n",
    "        print(f\"Cleaning phones and postals in {ds_name}...\")\n",
    "        dataframes[ds_name] = clean_phone_postal(df)\n",
    "\n",
    "print(\"Phones and postal codes cleaned.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "9509b6dd-4a75-4a66-b89c-667b1f80433a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standardizing company names in Customer...\n",
      "Standardizing company names in Customer_Errors...\n",
      "Company names standardized.\n"
     ]
    }
   ],
   "source": [
    "#Company Name Standardization\n",
    "\n",
    "import string\n",
    "\n",
    "# Company Name Standardization\n",
    "def standardize_company(df):\n",
    "    if 'Company' in df.columns:\n",
    "        df['Company'] = df['Company'].astype(str).str.lower().str.strip()\n",
    "        df['Company'] = df['Company'].apply(lambda x: x.translate(str.maketrans('', '', string.punctuation)))\n",
    "    return df\n",
    "\n",
    "for ds_name in ['Customer', 'Customer_Errors']:\n",
    "    df = dataframes.get(ds_name)\n",
    "    if df is not None:\n",
    "        print(f\"Standardizing company names in {ds_name}...\")\n",
    "        dataframes[ds_name] = standardize_company(df)\n",
    "\n",
    "print(\"Company names standardized.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "c2cf0b81-77c9-48c6-a8d7-04b4998bc6b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standardizing states in Customer...\n",
      "Standardizing states in Employee...\n",
      "Standardizing states in Invoice...\n",
      "States standardized.\n"
     ]
    }
   ],
   "source": [
    "#State Standardization\n",
    "\n",
    "state_map = {\n",
    "    'California': 'CA',\n",
    "    'New York': 'NY',\n",
    "    'Texas': 'TX',\n",
    "    # Add full mapping as needed\n",
    "}\n",
    "\n",
    "def standardize_state(df):\n",
    "    if 'State' in df.columns:\n",
    "        df['State'] = df['State'].map(state_map).fillna(df['State'])\n",
    "    return df\n",
    "\n",
    "for ds_name in ['Customer', 'Employee', 'Invoice']:\n",
    "    df = dataframes.get(ds_name)\n",
    "    if df is not None:\n",
    "        print(f\"Standardizing states in {ds_name}...\")\n",
    "        dataframes[ds_name] = standardize_state(df)\n",
    "\n",
    "print(\"States standardized.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "d87c7a15-0eb3-4780-a6f1-89df88b17c8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Special characters removed from text columns.\n"
     ]
    }
   ],
   "source": [
    "#Remove Special Characters From Text\n",
    "\n",
    "def remove_special_chars(df, cols):\n",
    "    for col in cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].astype(str).str.replace(r'[^\\x00-\\x7F]+', '', regex=True)\n",
    "    return df\n",
    "\n",
    "for name, df in dataframes.items():\n",
    "    dataframes[name] = remove_special_chars(df, text_columns)\n",
    "\n",
    "print(\"Special characters removed from text columns.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
