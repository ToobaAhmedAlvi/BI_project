{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0b6e0cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating cleaning stage if not exists...\n",
      "Creating cleaned schema if not exists...\n",
      "\n",
      "Processing table: Customer\n",
      "Loading raw data from Customer.csv ...\n",
      "Loaded 59 rows.\n",
      "Cleaned 59 rows.\n",
      "Exported cleaned data to ERD_cleaned\\Customer_cleaned.csv\n",
      "Removing old staged file Customer_cleaned.csv.gz ...\n",
      "Uploading E:/IBA_MS_DS 2026/Data WareHousing and Analysis/BI_project/ERD_cleaned/Customer_cleaned.csv to stage CHINOOK_DATABASE.ERD_SCHEMA.DATA_CLEANING_STAGE ...\n",
      "PUT result: [Row(source='Customer_cleaned.csv', target='Customer_cleaned.csv.gz', source_size=7093, target_size=4016, source_compression='NONE', target_compression='GZIP', status='UPLOADED', message='')]\n",
      "Creating cleaned table ERD_SCHEMA_CLEANED.CUSTOMER if not exists...\n",
      "Truncating table ERD_SCHEMA_CLEANED.CUSTOMER before loading fresh data...\n",
      "Copying data into ERD_SCHEMA_CLEANED.CUSTOMER from staged file Customer_cleaned.csv.gz ...\n",
      "COPY INTO result: [Row(file='data_cleaning_stage/Customer_cleaned.csv.gz', status='LOADED', rows_parsed=59, rows_loaded=59, error_limit=59, errors_seen=0, first_error=None, first_error_line=None, first_error_character=None, first_error_column_name=None)]\n",
      "Customer cleaned, uploaded, and loaded successfully.\n",
      "\n",
      "Processing table: Employee\n",
      "Loading raw data from Employee.csv ...\n",
      "Loaded 8 rows.\n",
      "Cleaned 8 rows.\n",
      "Exported cleaned data to ERD_cleaned\\Employee_cleaned.csv\n",
      "Removing old staged file Employee_cleaned.csv.gz ...\n",
      "Uploading E:/IBA_MS_DS 2026/Data WareHousing and Analysis/BI_project/ERD_cleaned/Employee_cleaned.csv to stage CHINOOK_DATABASE.ERD_SCHEMA.DATA_CLEANING_STAGE ...\n",
      "PUT result: [Row(source='Employee_cleaned.csv', target='Employee_cleaned.csv.gz', source_size=1401, target_size=704, source_compression='NONE', target_compression='GZIP', status='UPLOADED', message='')]\n",
      "Creating cleaned table ERD_SCHEMA_CLEANED.EMPLOYEE if not exists...\n",
      "Truncating table ERD_SCHEMA_CLEANED.EMPLOYEE before loading fresh data...\n",
      "Copying data into ERD_SCHEMA_CLEANED.EMPLOYEE from staged file Employee_cleaned.csv.gz ...\n",
      "COPY INTO result: [Row(file='data_cleaning_stage/Employee_cleaned.csv.gz', status='LOADED', rows_parsed=8, rows_loaded=8, error_limit=8, errors_seen=0, first_error=None, first_error_line=None, first_error_character=None, first_error_column_name=None)]\n",
      "Employee cleaned, uploaded, and loaded successfully.\n",
      "\n",
      "Processing table: Artist\n",
      "Loading raw data from Artist.csv ...\n",
      "Loaded 279 rows.\n",
      "Cleaned 271 rows.\n",
      "Exported cleaned data to ERD_cleaned\\Artist_cleaned.csv\n",
      "Removing old staged file Artist_cleaned.csv.gz ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_18688\\1824006095.py:115: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.drop_duplicates(inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading E:/IBA_MS_DS 2026/Data WareHousing and Analysis/BI_project/ERD_cleaned/Artist_cleaned.csv to stage CHINOOK_DATABASE.ERD_SCHEMA.DATA_CLEANING_STAGE ...\n",
      "PUT result: [Row(source='Artist_cleaned.csv', target='Artist_cleaned.csv.gz', source_size=7174, target_size=3808, source_compression='NONE', target_compression='GZIP', status='UPLOADED', message='')]\n",
      "Creating cleaned table ERD_SCHEMA_CLEANED.ARTIST if not exists...\n",
      "Truncating table ERD_SCHEMA_CLEANED.ARTIST before loading fresh data...\n",
      "Copying data into ERD_SCHEMA_CLEANED.ARTIST from staged file Artist_cleaned.csv.gz ...\n",
      "COPY INTO result: [Row(file='data_cleaning_stage/Artist_cleaned.csv.gz', status='LOADED', rows_parsed=271, rows_loaded=271, error_limit=271, errors_seen=0, first_error=None, first_error_line=None, first_error_character=None, first_error_column_name=None)]\n",
      "Artist cleaned, uploaded, and loaded successfully.\n",
      "\n",
      "Processing table: Album\n",
      "Loading raw data from Album.csv ...\n",
      "Loaded 352 rows.\n",
      "Cleaned 347 rows.\n",
      "Exported cleaned data to ERD_cleaned\\Album_cleaned.csv\n",
      "Removing old staged file Album_cleaned.csv.gz ...\n",
      "Uploading E:/IBA_MS_DS 2026/Data WareHousing and Analysis/BI_project/ERD_cleaned/Album_cleaned.csv to stage CHINOOK_DATABASE.ERD_SCHEMA.DATA_CLEANING_STAGE ...\n",
      "PUT result: [Row(source='Album_cleaned.csv', target='Album_cleaned.csv.gz', source_size=11781, target_size=5728, source_compression='NONE', target_compression='GZIP', status='UPLOADED', message='')]\n",
      "Creating cleaned table ERD_SCHEMA_CLEANED.ALBUM if not exists...\n",
      "Truncating table ERD_SCHEMA_CLEANED.ALBUM before loading fresh data...\n",
      "Copying data into ERD_SCHEMA_CLEANED.ALBUM from staged file Album_cleaned.csv.gz ...\n",
      "COPY INTO result: [Row(file='data_cleaning_stage/Album_cleaned.csv.gz', status='LOADED', rows_parsed=347, rows_loaded=347, error_limit=347, errors_seen=0, first_error=None, first_error_line=None, first_error_character=None, first_error_column_name=None)]\n",
      "Album cleaned, uploaded, and loaded successfully.\n",
      "\n",
      "Processing table: Invoice\n",
      "Loading raw data from Invoice.csv ...\n",
      "Loaded 412 rows.\n",
      "Cleaned 412 rows.\n",
      "Exported cleaned data to ERD_cleaned\\Invoice_cleaned.csv\n",
      "Removing old staged file Invoice_cleaned.csv.gz ...\n",
      "Uploading E:/IBA_MS_DS 2026/Data WareHousing and Analysis/BI_project/ERD_cleaned/Invoice_cleaned.csv to stage CHINOOK_DATABASE.ERD_SCHEMA.DATA_CLEANING_STAGE ...\n",
      "PUT result: [Row(source='Invoice_cleaned.csv', target='Invoice_cleaned.csv.gz', source_size=30372, target_size=6256, source_compression='NONE', target_compression='GZIP', status='UPLOADED', message='')]\n",
      "Creating cleaned table ERD_SCHEMA_CLEANED.INVOICE if not exists...\n",
      "Truncating table ERD_SCHEMA_CLEANED.INVOICE before loading fresh data...\n",
      "Copying data into ERD_SCHEMA_CLEANED.INVOICE from staged file Invoice_cleaned.csv.gz ...\n",
      "COPY INTO result: [Row(file='data_cleaning_stage/Invoice_cleaned.csv.gz', status='LOADED', rows_parsed=412, rows_loaded=412, error_limit=412, errors_seen=0, first_error=None, first_error_line=None, first_error_character=None, first_error_column_name=None)]\n",
      "Invoice cleaned, uploaded, and loaded successfully.\n",
      "\n",
      "Processing table: InvoiceLine\n",
      "Loading raw data from InvoiceLine.csv ...\n",
      "Loaded 2240 rows.\n",
      "Cleaned 2240 rows.\n",
      "Exported cleaned data to ERD_cleaned\\InvoiceLine_cleaned.csv\n",
      "Removing old staged file InvoiceLine_cleaned.csv.gz ...\n",
      "Uploading E:/IBA_MS_DS 2026/Data WareHousing and Analysis/BI_project/ERD_cleaned/InvoiceLine_cleaned.csv to stage CHINOOK_DATABASE.ERD_SCHEMA.DATA_CLEANING_STAGE ...\n",
      "PUT result: [Row(source='InvoiceLine_cleaned.csv', target='InvoiceLine_cleaned.csv.gz', source_size=46914, target_size=12192, source_compression='NONE', target_compression='GZIP', status='UPLOADED', message='')]\n",
      "Creating cleaned table ERD_SCHEMA_CLEANED.INVOICELINE if not exists...\n",
      "Truncating table ERD_SCHEMA_CLEANED.INVOICELINE before loading fresh data...\n",
      "Copying data into ERD_SCHEMA_CLEANED.INVOICELINE from staged file InvoiceLine_cleaned.csv.gz ...\n",
      "COPY INTO result: [Row(file='data_cleaning_stage/InvoiceLine_cleaned.csv.gz', status='LOADED', rows_parsed=2240, rows_loaded=2240, error_limit=2240, errors_seen=0, first_error=None, first_error_line=None, first_error_character=None, first_error_column_name=None)]\n",
      "InvoiceLine cleaned, uploaded, and loaded successfully.\n",
      "\n",
      "Processing table: Genre\n",
      "Loading raw data from Genre.csv ...\n",
      "Loaded 25 rows.\n",
      "Cleaned 25 rows.\n",
      "Exported cleaned data to ERD_cleaned\\Genre_cleaned.csv\n",
      "Removing old staged file Genre_cleaned.csv.gz ...\n",
      "Uploading E:/IBA_MS_DS 2026/Data WareHousing and Analysis/BI_project/ERD_cleaned/Genre_cleaned.csv to stage CHINOOK_DATABASE.ERD_SCHEMA.DATA_CLEANING_STAGE ...\n",
      "PUT result: [Row(source='Genre_cleaned.csv', target='Genre_cleaned.csv.gz', source_size=354, target_size=304, source_compression='NONE', target_compression='GZIP', status='UPLOADED', message='')]\n",
      "Creating cleaned table ERD_SCHEMA_CLEANED.GENRE if not exists...\n",
      "Truncating table ERD_SCHEMA_CLEANED.GENRE before loading fresh data...\n",
      "Copying data into ERD_SCHEMA_CLEANED.GENRE from staged file Genre_cleaned.csv.gz ...\n",
      "COPY INTO result: [Row(file='data_cleaning_stage/Genre_cleaned.csv.gz', status='LOADED', rows_parsed=25, rows_loaded=25, error_limit=25, errors_seen=0, first_error=None, first_error_line=None, first_error_character=None, first_error_column_name=None)]\n",
      "Genre cleaned, uploaded, and loaded successfully.\n",
      "\n",
      "Processing table: MediaType\n",
      "Loading raw data from MediaType.csv ...\n",
      "Loaded 5 rows.\n",
      "Cleaned 5 rows.\n",
      "Exported cleaned data to ERD_cleaned\\MediaType_cleaned.csv\n",
      "Removing old staged file MediaType_cleaned.csv.gz ...\n",
      "Uploading E:/IBA_MS_DS 2026/Data WareHousing and Analysis/BI_project/ERD_cleaned/MediaType_cleaned.csv to stage CHINOOK_DATABASE.ERD_SCHEMA.DATA_CLEANING_STAGE ...\n",
      "PUT result: [Row(source='MediaType_cleaned.csv', target='MediaType_cleaned.csv.gz', source_size=142, target_size=128, source_compression='NONE', target_compression='GZIP', status='UPLOADED', message='')]\n",
      "Creating cleaned table ERD_SCHEMA_CLEANED.MEDIATYPE if not exists...\n",
      "Truncating table ERD_SCHEMA_CLEANED.MEDIATYPE before loading fresh data...\n",
      "Copying data into ERD_SCHEMA_CLEANED.MEDIATYPE from staged file MediaType_cleaned.csv.gz ...\n",
      "COPY INTO result: [Row(file='data_cleaning_stage/MediaType_cleaned.csv.gz', status='LOADED', rows_parsed=5, rows_loaded=5, error_limit=5, errors_seen=0, first_error=None, first_error_line=None, first_error_character=None, first_error_column_name=None)]\n",
      "MediaType cleaned, uploaded, and loaded successfully.\n",
      "\n",
      "Processing table: Playlist\n",
      "Loading raw data from Playlist.csv ...\n",
      "Loaded 18 rows.\n",
      "Cleaned 18 rows.\n",
      "Exported cleaned data to ERD_cleaned\\Playlist_cleaned.csv\n",
      "Removing old staged file Playlist_cleaned.csv.gz ...\n",
      "Uploading E:/IBA_MS_DS 2026/Data WareHousing and Analysis/BI_project/ERD_cleaned/Playlist_cleaned.csv to stage CHINOOK_DATABASE.ERD_SCHEMA.DATA_CLEANING_STAGE ...\n",
      "PUT result: [Row(source='Playlist_cleaned.csv', target='Playlist_cleaned.csv.gz', source_size=317, target_size=256, source_compression='NONE', target_compression='GZIP', status='UPLOADED', message='')]\n",
      "Creating cleaned table ERD_SCHEMA_CLEANED.PLAYLIST if not exists...\n",
      "Truncating table ERD_SCHEMA_CLEANED.PLAYLIST before loading fresh data...\n",
      "Copying data into ERD_SCHEMA_CLEANED.PLAYLIST from staged file Playlist_cleaned.csv.gz ...\n",
      "COPY INTO result: [Row(file='data_cleaning_stage/Playlist_cleaned.csv.gz', status='LOADED', rows_parsed=18, rows_loaded=18, error_limit=18, errors_seen=0, first_error=None, first_error_line=None, first_error_character=None, first_error_column_name=None)]\n",
      "Playlist cleaned, uploaded, and loaded successfully.\n",
      "\n",
      "Processing table: PlaylistTrack\n",
      "Loading raw data from PlaylistTrack.csv ...\n",
      "Loaded 8715 rows.\n",
      "Cleaned 8715 rows.\n",
      "Exported cleaned data to ERD_cleaned\\PlaylistTrack_cleaned.csv\n",
      "Removing old staged file PlaylistTrack_cleaned.csv.gz ...\n",
      "Uploading E:/IBA_MS_DS 2026/Data WareHousing and Analysis/BI_project/ERD_cleaned/PlaylistTrack_cleaned.csv to stage CHINOOK_DATABASE.ERD_SCHEMA.DATA_CLEANING_STAGE ...\n",
      "PUT result: [Row(source='PlaylistTrack_cleaned.csv', target='PlaylistTrack_cleaned.csv.gz', source_size=67423, target_size=21040, source_compression='NONE', target_compression='GZIP', status='UPLOADED', message='')]\n",
      "Creating cleaned table ERD_SCHEMA_CLEANED.PLAYLISTTRACK if not exists...\n",
      "Truncating table ERD_SCHEMA_CLEANED.PLAYLISTTRACK before loading fresh data...\n",
      "Copying data into ERD_SCHEMA_CLEANED.PLAYLISTTRACK from staged file PlaylistTrack_cleaned.csv.gz ...\n",
      "COPY INTO result: [Row(file='data_cleaning_stage/PlaylistTrack_cleaned.csv.gz', status='LOADED', rows_parsed=8715, rows_loaded=8715, error_limit=8715, errors_seen=0, first_error=None, first_error_line=None, first_error_character=None, first_error_column_name=None)]\n",
      "PlaylistTrack cleaned, uploaded, and loaded successfully.\n",
      "\n",
      "Processing table: Track\n",
      "Loading raw data from Track.csv ...\n",
      "Loaded 3503 rows.\n",
      "Cleaned 3503 rows.\n",
      "Exported cleaned data to ERD_cleaned\\Track_cleaned.csv\n",
      "Removing old staged file Track_cleaned.csv.gz ...\n",
      "Uploading E:/IBA_MS_DS 2026/Data WareHousing and Analysis/BI_project/ERD_cleaned/Track_cleaned.csv to stage CHINOOK_DATABASE.ERD_SCHEMA.DATA_CLEANING_STAGE ...\n",
      "PUT result: [Row(source='Track_cleaned.csv', target='Track_cleaned.csv.gz', source_size=254045, target_size=92144, source_compression='NONE', target_compression='GZIP', status='UPLOADED', message='')]\n",
      "Creating cleaned table ERD_SCHEMA_CLEANED.TRACK if not exists...\n",
      "Truncating table ERD_SCHEMA_CLEANED.TRACK before loading fresh data...\n",
      "Copying data into ERD_SCHEMA_CLEANED.TRACK from staged file Track_cleaned.csv.gz ...\n",
      "COPY INTO result: [Row(file='data_cleaning_stage/Track_cleaned.csv.gz', status='LOADED', rows_parsed=3503, rows_loaded=3503, error_limit=3503, errors_seen=0, first_error=None, first_error_line=None, first_error_character=None, first_error_column_name=None)]\n",
      "Track cleaned, uploaded, and loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pathlib\n",
    "import pandas as pd\n",
    "from snowflake.snowpark import Session\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Snowflake connection parameters from env variables\n",
    "snowflake_params = {\n",
    "    \"account\": os.getenv(\"SNOWFLAKE_ACCOUNT\"),\n",
    "    \"user\": os.getenv(\"SNOWFLAKE_USER\"),\n",
    "    \"password\": os.getenv(\"SNOWFLAKE_PASSWORD\"),\n",
    "    \"role\": os.getenv(\"SNOWFLAKE_ROLE\"),\n",
    "    \"warehouse\": os.getenv(\"SNOWFLAKE_WAREHOUSE\"),\n",
    "    \"database\": os.getenv(\"SNOWFLAKE_DATABASE\"),\n",
    "    \"schema\": os.getenv(\"SNOWFLAKE_SCHEMA\"),  # Raw schema for stage\n",
    "}\n",
    "\n",
    "new_database = snowflake_params[\"database\"]\n",
    "raw_schema = snowflake_params[\"schema\"]\n",
    "cleaned_schema = \"ERD_SCHEMA_CLEANED\"\n",
    "cleaning_stage = \"DATA_CLEANING_STAGE\"\n",
    "\n",
    "# Reusable location lookup dictionary\n",
    "location_lookup = {\n",
    "    (\"Stuttgart\", \"Germany\"): {\"State\": \"Baden-Württemberg\", \"PostalCode\": \"70174\"},\n",
    "    (\"Oslo\", \"Norway\"): {\"State\": \"Oslo\", \"PostalCode\": \"0171\"},\n",
    "    (\"Prague\", \"Czech Republic\"): {\"State\": \"Prague\", \"PostalCode\": \"14700\"},\n",
    "    (\"Vienna\", \"Austria\"): {\"State\": \"Vienna\", \"PostalCode\": \"1010\"},\n",
    "    (\"Brussels\", \"Belgium\"): {\"State\": \"Brussels-Capital\", \"PostalCode\": \"1000\"},\n",
    "    (\"Copenhagen\", \"Denmark\"): {\"State\": \"Hovedstaden\", \"PostalCode\": \"1720\"},\n",
    "    (\"Lisbon\", \"Portugal\"): {\"State\": \"Lisbon\", \"PostalCode\": \"1100-042\"},\n",
    "    (\"Porto\", \"Portugal\"): {\"State\": \"Porto\", \"PostalCode\": \"4350-414\"},\n",
    "    (\"Berlin\", \"Germany\"): {\"State\": \"Berlin\", \"PostalCode\": \"10789\"},\n",
    "    (\"Frankfurt\", \"Germany\"): {\"State\": \"Hesse\", \"PostalCode\": \"60316\"},\n",
    "    (\"Paris\", \"France\"): {\"State\": \"Île-de-France\", \"PostalCode\": \"75009\"},\n",
    "    (\"Lyon\", \"France\"): {\"State\": \"Auvergne-Rhône-Alpes\", \"PostalCode\": \"69002\"},\n",
    "    (\"Bordeaux\", \"France\"): {\"State\": \"Nouvelle-Aquitaine\", \"PostalCode\": \"33000\"},\n",
    "    (\"Dijon\", \"France\"): {\"State\": \"Bourgogne-Franche-Comté\", \"PostalCode\": \"21000\"},\n",
    "    (\"Helsinki\", \"Finland\"): {\"State\": \"Uusimaa\", \"PostalCode\": \"00530\"},\n",
    "    (\"Budapest\", \"Hungary\"): {\"State\": \"Budapest\", \"PostalCode\": \"1073\"},\n",
    "    (\"Warsaw\", \"Poland\"): {\"State\": \"Mazowieckie\", \"PostalCode\": \"00-358\"},\n",
    "    (\"Madrid\", \"Spain\"): {\"State\": \"Community of Madrid\", \"PostalCode\": \"28015\"},\n",
    "    (\"Stockholm\", \"Sweden\"): {\"State\": \"SC\", \"PostalCode\": \"11230\"},\n",
    "    (\"London\", \"United Kingdom\"): {\"State\": \"England\", \"PostalCode\": \"N1 5LH\"},\n",
    "    (\"Edinburgh\", \"United Kingdom\"): {\"State\": \"Scotland\", \"PostalCode\": \"EH4 1HH\"},\n",
    "    (\"Buenos Aires\", \"Argentina\"): {\"State\": \"BA\", \"PostalCode\": \"1106\"},\n",
    "    (\"Santiago\", \"Chile\"): {\"State\": \"SA\", \"PostalCode\": \"8320000\"},\n",
    "    (\"Dublin\", \"Ireland\"): {\"State\": \"Dublin\", \"PostalCode\": \"D02 A529\"},\n",
    "    (\"Delhi\", \"India\"): {\"State\": \"Delhi\", \"PostalCode\": \"110017\"},\n",
    "    (\"Bangalore\", \"India\"): {\"State\": \"Karnataka\", \"PostalCode\": \"560001\"},\n",
    "     (\"Vienna\", \"Austria\"): {\"State\": \"Vienna\", \"PostalCode\": \"1010\"}\n",
    "}\n",
    "\n",
    "# ==== Cleaning functions ====\n",
    "\n",
    "def fill_state(city, country, current_state):\n",
    "    key = (city.strip() if pd.notnull(city) else None, country.strip() if pd.notnull(country) else None)\n",
    "    if not current_state or pd.isna(current_state) or str(current_state).strip() == \"\":\n",
    "        return location_lookup.get(key, {}).get(\"State\", current_state)\n",
    "    return current_state\n",
    "\n",
    "def fill_postalcode(city, country, current_postal):\n",
    "    key = (city.strip() if pd.notnull(city) else None, country.strip() if pd.notnull(country) else None)\n",
    "    if not current_postal or pd.isna(current_postal) or str(current_postal).strip() == \"\":\n",
    "        return location_lookup.get(key, {}).get(\"PostalCode\", current_postal)\n",
    "    return current_postal\n",
    "\n",
    "def clean_customer(df):\n",
    "    df.columns = df.columns.str.strip().str.upper()\n",
    "    df['STATE'] = df.apply(lambda row: fill_state(row['CITY'], row['COUNTRY'], row['STATE']), axis=1)\n",
    "    df['POSTALCODE'] = df.apply(lambda row: fill_postalcode(row['CITY'], row['COUNTRY'], row['POSTALCODE']), axis=1)\n",
    "    df.fillna({'FAX': '', 'COMPANY': '', 'EMAIL': ''}, inplace=True)\n",
    "    df.drop_duplicates(inplace=True)\n",
    "    df = df[df['CUSTOMERID'].notnull()]\n",
    "    return df\n",
    "\n",
    "def clean_invoice(df):\n",
    "    df.columns = df.columns.str.strip().str.upper()\n",
    "    df['INVOICEDATE'] = pd.to_datetime(df['INVOICEDATE'], errors='coerce')\n",
    "    # Fill BILLINGSTATE and BILLINGPOSTALCODE similar to customer\n",
    "    df['BILLINGSTATE'] = df.apply(lambda row: fill_state(row.get('BILLINGCITY', None), row.get('BILLINGCOUNTRY', None), row['BILLINGSTATE']), axis=1)\n",
    "    df['BILLINGPOSTALCODE'] = df.apply(lambda row: fill_postalcode(row.get('BILLINGCITY', None), row.get('BILLINGCOUNTRY', None), row['BILLINGPOSTALCODE']), axis=1)\n",
    "    df.fillna({'BILLINGADDRESS': '', 'BILLINGCITY': '', 'BILLINGSTATE': '', 'BILLINGCOUNTRY': '', 'BILLINGPOSTALCODE': ''}, inplace=True)\n",
    "    df.drop_duplicates(inplace=True)\n",
    "    return df\n",
    "\n",
    "def clean_track(df):\n",
    "    df.columns = df.columns.str.strip().str.upper()\n",
    "    df['ALBUMID'] = pd.to_numeric(df['ALBUMID'], errors='coerce').fillna(0).astype(int)\n",
    "    df['MEDIATYPEID'] = pd.to_numeric(df['MEDIATYPEID'], errors='coerce').fillna(0).astype(int)\n",
    "    df['GENREID'] = pd.to_numeric(df['GENREID'], errors='coerce').fillna(0).astype(int)\n",
    "    df['MILLISECONDS'] = pd.to_numeric(df['MILLISECONDS'], errors='coerce').fillna(0).astype(int)\n",
    "    df['BYTES'] = pd.to_numeric(df['BYTES'], errors='coerce').fillna(0).astype(int)\n",
    "    df['UNITPRICE'] = pd.to_numeric(df['UNITPRICE'], errors='coerce').fillna(0.0)\n",
    "    # Replace null or empty COMPOSER with 'Anonymous'\n",
    "    df['COMPOSER'] = df['COMPOSER'].fillna('').apply(lambda x: x if str(x).strip() != '' else 'Anonymous')\n",
    "    df.drop_duplicates(inplace=True)\n",
    "    return df\n",
    "\n",
    "# The rest of cleaning functions unchanged\n",
    "\n",
    "def clean_employee(df):\n",
    "    df.columns = df.columns.str.strip().str.upper()\n",
    "    df['BIRTHDATE'] = pd.to_datetime(df['BIRTHDATE'], errors='coerce')\n",
    "    df['HIREDATE'] = pd.to_datetime(df['HIREDATE'], errors='coerce')\n",
    "    df.fillna({'PHONE': '', 'FAX': ''}, inplace=True)\n",
    "    df.drop_duplicates(inplace=True)\n",
    "    return df\n",
    "\n",
    "def clean_artist(df):\n",
    "    df.columns = df.columns.str.strip().str.upper()\n",
    "    df = df[df['NAME'].notnull()]\n",
    "    df.drop_duplicates(inplace=True)\n",
    "    return df\n",
    "\n",
    "def clean_album(df):\n",
    "    df.columns = df.columns.str.strip().str.upper()\n",
    "    df['ARTISTID'] = pd.to_numeric(df['ARTISTID'], errors='coerce').fillna(0).astype(int)\n",
    "    df.drop_duplicates(inplace=True)\n",
    "    return df\n",
    "\n",
    "def clean_invoiceline(df):\n",
    "    df.columns = df.columns.str.strip().str.upper()\n",
    "    df['UNITPRICE'] = pd.to_numeric(df['UNITPRICE'], errors='coerce').fillna(0.0)\n",
    "    df['QUANTITY'] = pd.to_numeric(df['QUANTITY'], errors='coerce').fillna(0).astype(int)\n",
    "    df.drop_duplicates(inplace=True)\n",
    "    return df\n",
    "\n",
    "def clean_genre(df):\n",
    "    df.columns = df.columns.str.strip().str.upper()\n",
    "    df = df[df['NAME'].notnull()]\n",
    "    df.drop_duplicates(inplace=True)\n",
    "    return df\n",
    "\n",
    "def clean_mediatype(df):\n",
    "    df.columns = df.columns.str.strip().str.upper()\n",
    "    df = df[df['NAME'].notnull()]\n",
    "    df.drop_duplicates(inplace=True)\n",
    "    return df\n",
    "\n",
    "def clean_playlist(df):\n",
    "    df.columns = df.columns.str.strip().str.upper()\n",
    "    df = df[df['NAME'].notnull()]\n",
    "    df.drop_duplicates(inplace=True)\n",
    "    return df\n",
    "\n",
    "def clean_playlisttrack(df):\n",
    "    df.columns = df.columns.str.strip().str.upper()\n",
    "    df['PLAYLISTID'] = pd.to_numeric(df['PLAYLISTID'], errors='coerce').fillna(0).astype(int)\n",
    "    df['TRACKID'] = pd.to_numeric(df['TRACKID'], errors='coerce').fillna(0).astype(int)\n",
    "    df.drop_duplicates(inplace=True)\n",
    "    return df\n",
    "\n",
    "# ==== Snowflake interaction functions (unchanged) ====\n",
    "\n",
    "def create_cleaning_stage(session):\n",
    "    sql = f'''\n",
    "        CREATE STAGE IF NOT EXISTS \"{new_database}\".\"{raw_schema}\".\"{cleaning_stage}\"\n",
    "        FILE_FORMAT = (TYPE = 'CSV' FIELD_OPTIONALLY_ENCLOSED_BY = '\\\"' SKIP_HEADER = 1)\n",
    "    '''\n",
    "    print(\"Creating cleaning stage if not exists...\")\n",
    "    session.sql(sql).collect()\n",
    "\n",
    "def create_cleaned_schema(session):\n",
    "    sql = f'CREATE SCHEMA IF NOT EXISTS \"{new_database}\".\"{cleaned_schema}\"'\n",
    "    print(\"Creating cleaned schema if not exists...\")\n",
    "    session.sql(sql).collect()\n",
    "\n",
    "def export_to_csv(df, table_name):\n",
    "    folder = \"ERD_cleaned\"\n",
    "    os.makedirs(folder, exist_ok=True)  # Create folder if not exists\n",
    "    filename = os.path.join(folder, f\"{table_name}_cleaned.csv\")\n",
    "    df.to_csv(filename, index=False)\n",
    "    print(f\"Exported cleaned data to {filename}\")\n",
    "    return filename\n",
    "\n",
    "\n",
    "def remove_file_from_stage(session, filename):\n",
    "    remove_sql = f\"REMOVE @\\\"{new_database}\\\".\\\"{raw_schema}\\\".\\\"{cleaning_stage}\\\"/{filename}.gz\"\n",
    "    print(f\"Removing old staged file {filename}.gz ...\")\n",
    "    session.sql(remove_sql).collect()\n",
    "\n",
    "def upload_to_stage(session, csv_file):\n",
    "    csv_path = pathlib.Path(csv_file).resolve().as_posix()\n",
    "    filename = pathlib.Path(csv_file).name\n",
    "    # Remove old staged file before upload to avoid PUT SKIPPED\n",
    "    remove_file_from_stage(session, filename)\n",
    "    put_sql = f\"PUT 'file://{csv_path}' @\\\"{new_database}\\\".\\\"{raw_schema}\\\".\\\"{cleaning_stage}\\\" AUTO_COMPRESS=TRUE\"\n",
    "    print(f\"Uploading {csv_path} to stage {new_database}.{raw_schema}.{cleaning_stage} ...\")\n",
    "    res = session.sql(put_sql).collect()\n",
    "    print(\"PUT result:\", res)\n",
    "\n",
    "def copy_into_cleaned_table(session, table_name, csv_file):\n",
    "    staged_filename = f\"{pathlib.Path(csv_file).name}.gz\"  # e.g. Album_cleaned.csv.gz\n",
    "    copy_sql = f'''\n",
    "        COPY INTO \"{cleaned_schema}\".\"{table_name.upper()}\"\n",
    "        FROM @\\\"{new_database}\\\".\\\"{raw_schema}\\\".\\\"{cleaning_stage}\\\"/{staged_filename}\n",
    "        FILE_FORMAT = (TYPE = 'CSV' FIELD_OPTIONALLY_ENCLOSED_BY='\\\"' SKIP_HEADER=1)\n",
    "        ON_ERROR = 'CONTINUE'\n",
    "        FORCE = TRUE\n",
    "    '''\n",
    "    print(f\"Copying data into {cleaned_schema}.{table_name.upper()} from staged file {staged_filename} ...\")\n",
    "    res = session.sql(copy_sql).collect()\n",
    "    print(\"COPY INTO result:\", res)\n",
    "\n",
    "def truncate_cleaned_table(session, table_name):\n",
    "    truncate_sql = f'TRUNCATE TABLE \"{cleaned_schema}\".\"{table_name.upper()}\"'\n",
    "    print(f\"Truncating table {cleaned_schema}.{table_name.upper()} before loading fresh data...\")\n",
    "    session.sql(truncate_sql).collect()\n",
    "\n",
    "def clean_data(df, table_name):\n",
    "    switcher = {\n",
    "        \"Customer\": clean_customer,\n",
    "        \"Employee\": clean_employee,\n",
    "        \"Artist\": clean_artist,\n",
    "        \"Album\": clean_album,\n",
    "        \"Invoice\": clean_invoice,\n",
    "        \"InvoiceLine\": clean_invoiceline,\n",
    "        \"Genre\": clean_genre,\n",
    "        \"MediaType\": clean_mediatype,\n",
    "        \"Playlist\": clean_playlist,\n",
    "        \"PlaylistTrack\": clean_playlisttrack,\n",
    "        \"Track\": clean_track,\n",
    "    }\n",
    "    func = switcher.get(table_name, lambda df: df)\n",
    "    return func(df)\n",
    "\n",
    "def create_cleaned_table_like_raw(session, table_name):\n",
    "    create_sql = f'''\n",
    "        CREATE TABLE IF NOT EXISTS \"{cleaned_schema}\".\"{table_name.upper()}\" LIKE \"{raw_schema}\".\"{table_name.upper()}\"\n",
    "    '''\n",
    "    print(f\"Creating cleaned table {cleaned_schema}.{table_name.upper()} if not exists...\")\n",
    "    session.sql(create_sql).collect()\n",
    "\n",
    "def main():\n",
    "    with Session.builder.configs(snowflake_params).create() as session:\n",
    "        create_cleaning_stage(session)\n",
    "        create_cleaned_schema(session)\n",
    "\n",
    "        tables = [\"Customer\", \"Employee\", \"Artist\", \"Album\", \"Invoice\", \"InvoiceLine\",\n",
    "                  \"Genre\", \"MediaType\", \"Playlist\", \"PlaylistTrack\", \"Track\"]\n",
    "\n",
    "        for table in tables:\n",
    "            print(f\"\\nProcessing table: {table}\")\n",
    "            csv_input = f\"{table}.csv\"\n",
    "            \n",
    "            if not os.path.exists(csv_input):\n",
    "                print(f\"ERROR: CSV file {csv_input} not found. Skipping {table}.\")\n",
    "                continue\n",
    "\n",
    "            print(f\"Loading raw data from {csv_input} ...\")\n",
    "            df = pd.read_csv(csv_input)\n",
    "            print(f\"Loaded {len(df)} rows.\")\n",
    "\n",
    "            cleaned_df = clean_data(df, table)\n",
    "            print(f\"Cleaned {len(cleaned_df)} rows.\")\n",
    "\n",
    "            csv_file = export_to_csv(cleaned_df, table)\n",
    "\n",
    "            upload_to_stage(session, csv_file)\n",
    "\n",
    "            create_cleaned_table_like_raw(session, table)\n",
    "\n",
    "            truncate_cleaned_table(session, table)\n",
    "\n",
    "            copy_into_cleaned_table(session, table, csv_file)\n",
    "\n",
    "            print(f\"{table} cleaned, uploaded, and loaded successfully.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "snowflake",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
